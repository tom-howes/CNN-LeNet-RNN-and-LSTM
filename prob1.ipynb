{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "919d4913",
      "metadata": {
        "origin_pos": 1,
        "id": "919d4913"
      },
      "source": [
        "# Convolutional Neural Networks (LeNet)\n",
        ":label:`sec_lenet`\n",
        "\n",
        "We now have all the ingredients required to assemble\n",
        "a fully-functional CNN.\n",
        "In our earlier encounter with image data, we applied\n",
        "a linear model with softmax regression (:numref:`sec_softmax_scratch`)\n",
        "and an MLP (:numref:`sec_mlp-implementation`)\n",
        "to pictures of clothing in the Fashion-MNIST dataset.\n",
        "To make such data amenable we first flattened each image from a $28\\times28$ matrix\n",
        "into a fixed-length $784$-dimensional vector,\n",
        "and thereafter processed them in fully connected layers.\n",
        "Now that we have a handle on convolutional layers,\n",
        "we can retain the spatial structure in our images.\n",
        "As an additional benefit of replacing fully connected layers with convolutional layers,\n",
        "we will enjoy more parsimonious models that require far fewer parameters.\n",
        "\n",
        "In this section, we will introduce *LeNet*,\n",
        "among the first published CNNs\n",
        "to capture wide attention for its performance on computer vision tasks.\n",
        "The model was introduced by (and named for) Yann LeCun,\n",
        "then a researcher at AT&T Bell Labs,\n",
        "for the purpose of recognizing handwritten digits in images :cite:`LeCun.Bottou.Bengio.ea.1998`.\n",
        "This work represented the culmination\n",
        "of a decade of research developing the technology;\n",
        "LeCun's team published the first study to successfully\n",
        "train CNNs via backpropagation :cite:`LeCun.Boser.Denker.ea.1989`.\n",
        "\n",
        "At the time LeNet achieved outstanding results\n",
        "matching the performance of support vector machines,\n",
        "then a dominant approach in supervised learning, achieving an error rate of less than 1% per digit.\n",
        "LeNet was eventually adapted to recognize digits\n",
        "for processing deposits in ATM machines.\n",
        "To this day, some ATMs still run the code\n",
        "that Yann LeCun and his colleague Leon Bottou wrote in the 1990s!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2cc6adfb",
      "metadata": {
        "id": "2cc6adfb"
      },
      "outputs": [],
      "source": [
        "# Colab-compatible d2l minimal replacement\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class DataModule:\n",
        "    def save_hyperparameters(self): pass\n",
        "    def get_dataloader(self, train): raise NotImplementedError\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "    def save_hyperparameters(self): pass\n",
        "    def forward(self, X): return self.net(X)\n",
        "    def apply_init(self, inputs, init_fn):\n",
        "        with torch.no_grad(): self.net(*inputs)\n",
        "        self.net.apply(init_fn)\n",
        "        return self\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(self, max_epochs=10, num_gpus=0):\n",
        "        self.max_epochs = max_epochs\n",
        "        self.num_gpus = num_gpus\n",
        "        self.show_progress = False\n",
        "    def fit(self, model, data): print(\"Use custom training loop\")\n",
        "\n",
        "class ProgressBoard:\n",
        "    def __init__(self):\n",
        "        self.x, self.xlabel = 'epoch', 'epoch'\n",
        "        self.fig, self.ax = plt.subplots(figsize=(8, 6))\n",
        "        self.data = {}\n",
        "    def draw(self, x, y, label):\n",
        "        if label not in self.data: self.data[label] = {'x': [], 'y': []}\n",
        "        self.data[label]['x'].append(x)\n",
        "        self.data[label]['y'].append(y)\n",
        "        self.ax.clear()\n",
        "        for lbl, vals in self.data.items():\n",
        "            self.ax.plot(vals['x'], vals['y'], label=lbl, marker='o')\n",
        "        self.ax.set_xlabel(self.xlabel)\n",
        "        self.ax.legend()\n",
        "        self.ax.grid(True)\n",
        "        plt.tight_layout()\n",
        "\n",
        "def try_gpu(i=0):\n",
        "    return torch.device(f'cuda:{i}' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def add_to_class(Class):\n",
        "    def wrapper(obj):\n",
        "        setattr(Class, obj.__name__, obj)\n",
        "        return obj\n",
        "    return wrapper\n",
        "\n",
        "# Create a module-like object\n",
        "class d2l_module:\n",
        "    DataModule = DataModule\n",
        "    Classifier = Classifier\n",
        "    Trainer = Trainer\n",
        "    ProgressBoard = ProgressBoard\n",
        "    try_gpu = staticmethod(try_gpu)\n",
        "    add_to_class = staticmethod(add_to_class)\n",
        "\n",
        "d2l = d2l_module()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08e44e08",
      "metadata": {
        "origin_pos": 3,
        "tab": [
          "pytorch"
        ],
        "id": "08e44e08"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46cde4b3",
      "metadata": {
        "id": "46cde4b3"
      },
      "outputs": [],
      "source": [
        "\n",
        "class CIFAR10Data(d2l.DataModule):  #@save\n",
        "    \"\"\"Minimal CIFAR-10 DataModule compatible with d2l.Trainer.\"\"\"\n",
        "    def __init__(self, batch_size=128, num_workers=4, root=\"../data\", aug=True):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        normalize = transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n",
        "        if aug:\n",
        "            self.train_tfms = transforms.Compose([\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.RandomCrop(32, padding=4),\n",
        "                transforms.ToTensor(),\n",
        "                normalize,\n",
        "            ])\n",
        "        else:\n",
        "            self.train_tfms = transforms.Compose([transforms.ToTensor(), normalize])\n",
        "        self.test_tfms = transforms.Compose([transforms.ToTensor(), normalize])\n",
        "        self.batch_size = batch_size\n",
        "        self.num_workers = num_workers\n",
        "        self.train = datasets.CIFAR10(root=root, train=True, download=True, transform=self.train_tfms)\n",
        "        self.val = datasets.CIFAR10(root=root, train=False, download=True, transform=self.test_tfms)\n",
        "\n",
        "    def get_dataloader(self, train):\n",
        "        ds = self.train if train else self.val\n",
        "        return DataLoader(ds, batch_size=self.batch_size, shuffle=train, num_workers=self.num_workers, pin_memory=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3bef277",
      "metadata": {
        "id": "f3bef277"
      },
      "outputs": [],
      "source": [
        "\n",
        "class MNISTData(d2l.DataModule):  #@save\n",
        "    \"\"\"Minimal MNIST DataModule compatible with d2l.Trainer.\"\"\"\n",
        "    def __init__(self, batch_size=128, num_workers=4, root=\"../data\", aug=True):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        mnist_mean, mnist_std = 0.1307, 0.3081\n",
        "        normalize = transforms.Normalize((mnist_mean,), (mnist_std,))\n",
        "        if aug:\n",
        "            self.train_tfms = transforms.Compose([\n",
        "                transforms.Resize(32),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.RandomCrop(32, padding=4),\n",
        "                transforms.ToTensor(),\n",
        "                normalize,\n",
        "            ])\n",
        "        else:\n",
        "            self.train_tfms = transforms.Compose([transforms.Resize(32), transforms.ToTensor(), normalize])\n",
        "        self.test_tfms = transforms.Compose([transforms.Resize(32),transforms.ToTensor(), normalize])\n",
        "        self.batch_size = batch_size\n",
        "        self.num_workers = num_workers\n",
        "        self.train = datasets.MNIST(root=root, train=True, download=True, transform=self.train_tfms)\n",
        "        self.val = datasets.MNIST(root=root, train=False, download=True, transform=self.test_tfms)\n",
        "\n",
        "    def get_dataloader(self, train):\n",
        "        ds = self.train if train else self.val\n",
        "        return DataLoader(ds, batch_size=self.batch_size, shuffle=train, num_workers=self.num_workers, pin_memory=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b877766d",
      "metadata": {
        "id": "b877766d"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class LazyConv2d_scratch(nn.Module):  # @save\n",
        "    \"\"\"Conv2d from scratch with lazy in_channels. No torch.nn.functional usage.\n",
        "    Supports groups, stride, padding, dilation, bias.\"\"\"\n",
        "    def __init__(self, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True):\n",
        "        super().__init__()\n",
        "        kernel_size = (kernel_size, kernel_size)\n",
        "        stride      = (stride, stride)\n",
        "        padding     = (padding, padding)\n",
        "        dilation    = (dilation, dilation)\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size  = kernel_size\n",
        "        self.stride       = stride\n",
        "        self.padding      = padding\n",
        "        self.dilation     = dilation\n",
        "        self.groups       = groups\n",
        "        self.bias_flag    = bias\n",
        "        self.weight = None\n",
        "        self.bias   = None\n",
        "        self._initialized = False\n",
        "\n",
        "    def _init_params(self, in_channels: int, device=None, dtype=None):\n",
        "        assert in_channels % self.groups == 0, \"in_channels must be divisible by groups\"\n",
        "        kh, kw = self.kernel_size\n",
        "        w = torch.empty(self.out_channels, in_channels // self.groups, kh, kw, device=device, dtype=dtype)\n",
        "        nn.init.xavier_uniform_(w)\n",
        "        self.weight = nn.Parameter(w)\n",
        "        if self.bias_flag:\n",
        "            self.bias = nn.Parameter(torch.zeros(self.out_channels, device=device, dtype=dtype))\n",
        "        self._initialized = True\n",
        "\n",
        "    @staticmethod\n",
        "    def _pad2d(x, pad_h, pad_w):\n",
        "        if pad_h == 0 and pad_w == 0:\n",
        "            return x\n",
        "        N, C, H, W = x.shape\n",
        "        out = x.new_zeros((N, C, H + 2*pad_h, W + 2*pad_w))\n",
        "        out[:, :, pad_h:pad_h+H, pad_w:pad_w+W] = x\n",
        "        return out\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "\n",
        "        # IMPLEMENTATION ROADMAP (fill each TODO with code):\n",
        "\n",
        "        # Lazy init\n",
        "        if not self._initialized:\n",
        "            self._init_params(x.shape[1], device=x.device, dtype=x.dtype)\n",
        "\n",
        "        #1 TODO  Parse shapes / hyperparams\n",
        "        # Extract N, C, H, W from x.shape.\n",
        "        # Unpack (kh, kw), (sh, sw), (ph, pw), (dh, dw) from self.\n",
        "        # Let g = self.groups; compute c_per_g = C // g; o_per_g = self.out_channels // g.\n",
        "        # Add assertions:\n",
        "        #    * C % g == 0\n",
        "        #   * self.out_channels % g == 0\n",
        "        N, C, H, W = x.shape\n",
        "        kh, kw = self.kernel_size\n",
        "        sh, sw = self.stride\n",
        "        ph, pw = self.padding\n",
        "        dh, dw = self.dilation\n",
        "\n",
        "        g = self.groups\n",
        "        c_per_g = C // g\n",
        "        o_per_g = self.out_channels // g\n",
        "\n",
        "        assert C % g == 0, f\"Input channels ({C}) must be divisible by groups ({g})\"\n",
        "        assert self.out_channels % g == 0, f\"Output channels ({self.out_channels}) must be divisible by groups ({g})\"\n",
        "\n",
        "        #2 Effective kernel with dilation\n",
        "        eff_kh = (kh - 1) * dh + 1\n",
        "        eff_kw = (kw - 1) * dw + 1\n",
        "\n",
        "        #3 Padding\n",
        "        x_pad = self._pad2d(x, ph, pw)\n",
        "        _, _, Hp, Wp = x_pad.shape\n",
        "\n",
        "        #4  Output spatial dims\n",
        "        out_h = (Hp - eff_kh) // sh + 1\n",
        "        out_w = (Wp - eff_kw) // sw + 1\n",
        "        #Optionally assert out_h > 0 and out_w > 0\n",
        "\n",
        "        #5 Extract sliding windows (patches)\n",
        "        # Use Tensor.unfold twice:\n",
        "        patches = x_pad.unfold(dimension=2, size=eff_kh, step=sh).unfold(dimension=3, size=eff_kw, step=sw)\n",
        "        #Shape target :(N, C, out_h, out_w, eff_kh, eff_kw)\n",
        "\n",
        "        #TODO Handle dilation:\n",
        "        #if dh > 1 or dw > 1 → slice the last two dims with steps ::dh / ::dw\n",
        "        #Result shape target: (N, C, out_h, out_w, kh, kw)\n",
        "        if dh > 1 or dw > 1:\n",
        "            patches = patches[:, :, :, :, ::dh, ::dw]\n",
        "\n",
        "        #6 TODO Group the channels and flatten kernel window per location\n",
        "           # Use .reshape / .view to go to (N, g, c_per_g, out_h, out_w, kh, kw)\n",
        "           # Use .permute to reorder to (N, g, out_h, out_w, c_per_g, kh, kw)\n",
        "           # Use .reshape/.view to flatten kernel part → (N, g, out_h, out_w, c_per_g * kh * kw)\n",
        "           # Consider calling .contiguous() before any .view if you permuted.\n",
        "        patches = patches.reshape(N, g, c_per_g, out_h, out_w, kh, kw).permute(0, 1, 3, 4, 2, 5, 6).reshape(N, g, out_h, out_w, c_per_g * kh * kw)\n",
        "        #7 Prepare grouped weights\n",
        "        # Reshape weight to (g, o_per_g, c_per_g, kh, kw)\n",
        "        # Flatten kernel dims to (g, o_per_g, c_per_g * kh * kw)\n",
        "        # Consider .contiguous() to be safe before .view\n",
        "        Wg = self.weight.reshape(g, o_per_g, c_per_g, kh, kw).reshape(g, o_per_g, c_per_g*kh*kw)\n",
        "\n",
        "\n",
        "        #8 TODO Convolution math (one of):\n",
        "        #Option A: torch.einsum with signature 'nghwk,gok->nghwo' (patches: (N,g,out_h,out_w,K), Wg: (g,o_per_g,K))\n",
        "        out = torch.einsum('nghwk,gok->nghwo', patches, Wg)\n",
        "        #Option B: torch.bmm over a merged batch dimension:\n",
        "               # reshape patches to (N*g*out_h*out_w, K)\n",
        "               # Wg to (g, o_per_g, K) then expand/reindex appropriately\n",
        "               # multiply to get (N*g*out_h*out_w, o_per_g), then reshape back\n",
        "        #Target output shape after this step: (N, g, out_h, out_w, o_per_g)\n",
        "\n",
        "\n",
        "\n",
        "        #9 Merge groups to channels\n",
        "        # .permute to (N, o_per_g, out_h, out_w, g)\n",
        "        # .reshape to (N, self.out_channels, out_h, out_w)\n",
        "        out = out.permute(0,4,2,3,1).reshape(N, self.out_channels, out_h, out_w)\n",
        "        # Use .contiguous() if necessary before .view\n",
        "\n",
        "\n",
        "        #10 Add bias (if present)\n",
        "        if self.bias is not None:\n",
        "            out = out + self.bias.view(1, -1, 1, 1)\n",
        "\n",
        "\n",
        "        #11 Return\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "        #HINTS ON OPS TO USE:\n",
        "        #  - Tensor.unfold for sliding windows\n",
        "        #  - Tensor.permute / Tensor.reshape (or .view) / .contiguous\n",
        "        #  - torch.einsum OR torch.bmm for the actual multiply–accumulate\n",
        "        #  - Broadcasting rules for adding bias\n",
        "        #  - Prefer integer arithmetic for out_h/out_w (//)\n",
        "        #  - Keep dtype/device consistent with x and parameters\n",
        "\n",
        "        raise NotImplementedError(\"Fill in the steps described above.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9191bde1",
      "metadata": {
        "id": "9191bde1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from typing import Tuple\n",
        "\n",
        "class AvgPool2D_scratch(nn.Module):  # @save\n",
        "    \"\"\"AveragePool2d from scratch. No torch.nn.functional used.\n",
        "    Students: implement padding + pooling logic using basic tensor ops only.\"\"\"\n",
        "    def __init__(self,\n",
        "                 kernel_size,\n",
        "                 stride=None,\n",
        "                 padding=0,\n",
        "                 ceil_mode: bool = False,\n",
        "                 count_include_pad: bool = True):\n",
        "        super().__init__()\n",
        "        kernel_size = (kernel_size, kernel_size)\n",
        "        if stride is None: stride = kernel_size\n",
        "        if isinstance(stride, int): stride = (stride, stride)\n",
        "        padding = (padding, padding)\n",
        "\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        self.ceil_mode = ceil_mode\n",
        "        self.count_include_pad = count_include_pad\n",
        "\n",
        "    @staticmethod\n",
        "    def _pad2d(x: torch.Tensor, pad_h: int, pad_w: int):\n",
        "        if pad_h == 0 and pad_w == 0:\n",
        "            return x, (0,0,0,0)\n",
        "        N, C, H, W = x.shape\n",
        "        out = x.new_zeros((N, C, H + 2*pad_h, W + 2*pad_w))\n",
        "        out[:, :, pad_h:pad_h+H, pad_w:pad_w+W] = x\n",
        "        return out, (pad_h, pad_h, pad_w, pad_w)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "\n",
        "        #Average pooling from scratch.\n",
        "        #Inputs:\n",
        "        #  - x: NCHW tensor\n",
        "        #Behavior to implement:\n",
        "\n",
        "        #1 Unpack parameters:\n",
        "        #   - kh, kw = self.kernel_size\n",
        "        #   - sh, sw = self.stride\n",
        "        #   - ph, pw = self.padding\n",
        "        #   - Read flags: self.ceil_mode, self.count_include_pad\n",
        "        N, C, H, W = x.shape\n",
        "        kh, kw = self.kernel_size\n",
        "        sh, sw = self.stride\n",
        "        ph, pw = self.padding\n",
        "\n",
        "        #2 Padding:\n",
        "        #   - Produce x_pad using _pad2d(x, ph, pw).\n",
        "        #   - Record padded spatial sizes Hp, Wp.\n",
        "        x_pad, _ = self._pad2d(x, ph, pw)\n",
        "        _, _, Hp, Wp = x_pad.shape\n",
        "\n",
        "\n",
        "        #3 TODO Output spatial size:\n",
        "        #   - Compute out_h, out_w using stride and kernel sizes.\n",
        "        #   - If self.ceil_mode is False, use floor-style size.\n",
        "        #   - If self.ceil_mode is True, implement the ceil behavior:\n",
        "        #     (i.e., include a final partial step if stride steps don't fit exactly).\n",
        "        #     You may need to handle the last windows separately.\n",
        "\n",
        "        if not self.ceil_mode:\n",
        "            out_h = (H + 2 * ph - kh) // sh + 1\n",
        "            out_w = (W + 2 * pw - kw) // sw + 1\n",
        "        else:\n",
        "            out_h = (H + 2 * ph - kh + sh - 1) // sh + 1\n",
        "            out_w = (W + 2 * pw - kw + sw - 1) // sw + 1\n",
        "\n",
        "        #4 TODO  Extract sliding windows:\n",
        "        #   - For every top-left window position, gather a (kh, kw) patch per (N, C).\n",
        "        #   - You may use Tensor.unfold twice (over H then W) to create a view of windows.\n",
        "        #     Expected intermediate shape idea: (N, C, out_h, out_w, kh, kw)\n",
        "        patches = x_pad.unfold(dimension=2, size=kh, step=sh).unfold(dimension=3, size=kw, step=sw)\n",
        "\n",
        "        #5 Compute denominator for the mean:\n",
        "        #   - If count_include_pad is True: denom = kh * kw for all windows.\n",
        "        #   - If False: windows overlapping padded zeros should divide by the\n",
        "        #     number of **valid** (non-padded) elements actually inside the original image.\n",
        "        #    (You can compute a parallel “mask” with ones in the unpadded area and zeros in pad,\n",
        "        #      and average using sums / valid_counts.)\n",
        "        denom = (kh * kw)\n",
        "\n",
        "\n",
        "\n",
        "        #6 Compute pooled output:\n",
        "        # Sum values over the last two dims (kh, kw) to get window sums.\n",
        "        sums = patches.sum(dim=(-1, -2), keepdim=False)\n",
        "        # Divide by the appropriate denominator per window (see step 5).\n",
        "        # Final shape should be (N, C, out_h, out_w).\n",
        "        avg = sums / denom  # [N, C, out_h, out_w]\n",
        "\n",
        "\n",
        "        #7 Return the pooled tensor.\n",
        "        return avg\n",
        "\n",
        "        raise NotImplementedError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d5f5118",
      "metadata": {
        "id": "6d5f5118"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "def batch_norm(X, gamma, beta, moving_mean, moving_var, eps, momentum):\n",
        "    \"\"\"\n",
        "    Functional BatchNorm starter (instructions only).\n",
        "\n",
        "    Inputs:\n",
        "      - X:            Tensor of shape (N, C) for MLPs or (N, C, H, W) for convs\n",
        "      - gamma, beta:  Learnable affine params (per-channel). Handle broadcasting.\n",
        "      - moving_mean:  Running mean buffer (same broadcastable shape as X's channel dim)\n",
        "      - moving_var:   Running var  buffer (same)\n",
        "      - eps:          Small constant for numerical stability (e.g., 1e-5)\n",
        "      - momentum:     Update rate for running stats (e.g., 0.1)\n",
        "    \"\"\"\n",
        "\n",
        "    # Determine mode: Use is_grad_enabled to determine whether we are in training mode\n",
        "    if not torch.is_grad_enabled():\n",
        "        # In prediction mode, use mean and variance obtained by moving average\n",
        "        X_hat = (X - moving_mean) / torch.sqrt(moving_var + eps)\n",
        "\n",
        "    else:\n",
        "        assert len(X.shape) in (2, 4)\n",
        "        if len(X.shape) == 2: #If X.ndim == 2 (N, C): reduce over dim=0 → mean,var shapes (C,)\n",
        "            #TODO\n",
        "            mean = torch.mean(X, dim=0)\n",
        "            var = torch.var(X, dim=0, unbiased=False)\n",
        "            X_hat = (X - mean) / torch.sqrt(var + eps)\n",
        "\n",
        "        else: # If X.ndim == 4 (N, C, H, W): reduce over dims (0, 2, 3)\n",
        "            #Keep dimensions if you plan to broadcast directly over N,H,W\n",
        "            #  OR reshape later to match (C,) parameters.\n",
        "            #TODO\n",
        "            mean = torch.mean(X, dim=(0, 2, 3))\n",
        "            var = torch.var(X, dim=(0, 2, 3), unbiased=False)\n",
        "            mean_bc = mean.view(1, -1, 1, 1)\n",
        "            var_bc = var.view(1, -1, 1, 1)\n",
        "            X_hat = (X - mean_bc) / torch.sqrt(var_bc + eps)\n",
        "\n",
        "        # TODO  Update the mean and variance using moving average in TRAIN mode\n",
        "        # moving_mean ← (1 - momentum) * moving_mean + momentum * batch_mean\n",
        "        # moving_var  ← (1 - momentum) * moving_var  + momentum * batch_var\n",
        "        # Do these updates without tracking gradients.\n",
        "        with torch.no_grad():\n",
        "          # Ensure mean and var are reshaped to (1, C, 1, 1) for broadcasting\n",
        "          mean_to_add = mean.view_as(moving_mean)\n",
        "          var_to_add = var.view_as(moving_var)\n",
        "          moving_mean.mul_(1 - momentum).add_(mean_to_add * momentum)\n",
        "          moving_var.mul_(1 - momentum).add_(var_to_add * momentum)\n",
        "\n",
        "    # Affine transform:\n",
        "    Y = gamma * X_hat + beta\n",
        "\n",
        "    #Return Y along with the updated moving_mean and moving_var.\n",
        "    return Y, moving_mean.data, moving_var.data\n",
        "\n",
        "    #Notes:\n",
        "    #  - Be careful with shapes/broadcasting for 2D vs 4D.\n",
        "    #  - Keep numerical stability: add eps before sqrt.\n",
        "    #  - Do NOT call torch.nn.functional.batch_norm or nn.BatchNorm*.\n",
        "\n",
        "\n",
        "    raise NotImplementedError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43fab924",
      "metadata": {
        "id": "43fab924"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class BatchNorm(nn.Module):\n",
        "    # num_features: the number of outputs for a fully connected layer or the\n",
        "    # number of output channels for a convolutional layer. num_dims: 2 for a\n",
        "    # fully connected layer and 4 for a convolutional layer\n",
        "    def __init__(self, num_features, num_dims):\n",
        "        super().__init__()\n",
        "        if num_dims == 2:\n",
        "            shape = (1, num_features)\n",
        "        else:\n",
        "            shape = (1, num_features, 1, 1)\n",
        "        # The scale parameter and the shift parameter (model parameters) are\n",
        "        # initialized to 1 and 0, respectively\n",
        "        self.gamma = nn.Parameter(torch.ones(shape))\n",
        "        self.beta = nn.Parameter(torch.zeros(shape))\n",
        "        # The variables that are not model parameters are initialized to 0 and 1\n",
        "        self.moving_mean = torch.zeros(shape)\n",
        "        self.moving_var = torch.ones(shape)\n",
        "\n",
        "    def forward(self, X):\n",
        "        #Ensure self.moving_mean / self.moving_var are on X.device (move if needed).\n",
        "        if self.moving_mean.device != X.device:\n",
        "            self.moving_mean = self.moving_mean.to(X.device)\n",
        "            self.moving_var = self.moving_var.to(X.device)\n",
        "\n",
        "        #TODO (Training vs Eval behavior):\n",
        "\n",
        "        # If self.training is False (eval mode):\n",
        "        #    * Call your batch_norm(X, gamma, beta, moving_mean, moving_var, eps, momentum)\n",
        "        #      using the RUNNING stats without updating them.\n",
        "        #    * Make sure you DO NOT track gradients for running stats in eval (use no_grad or detach).\n",
        "        #    * Return the normalized+affine output Y.\n",
        "\n",
        "        # If self.training is True (training mode):\n",
        "        #      * Call your batch_norm with grads enabled so that gamma/beta get gradients.\n",
        "        #      * Inside batch_norm, compute batch mean/var and UPDATE running stats in-place\n",
        "        #        with the given momentum.\n",
        "        #      * Return Y.\n",
        "        #  - Pass reasonable defaults: eps=1e-5, momentum=0.1 (or make them attributes if desired).\n",
        "        if self.training:\n",
        "            Y, self.moving_mean, self.moving_var = batch_norm(X, self.gamma, self.beta, self.moving_mean, self.moving_var, eps=1e-5, momentum=0.1)\n",
        "        else:\n",
        "          with torch.no_grad():\n",
        "            Y, self.moving_mean, self.moving_var = batch_norm(X, self.gamma, self.beta, self.moving_mean, self.moving_var, eps=1e-5, momentum=0.1)\n",
        "\n",
        "        return Y #(same shape as X).\n",
        "\n",
        "        raise NotImplementedError(\"Implement forward by delegating to your batch_norm() with correct train/eval paths.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37d35ed9",
      "metadata": {
        "id": "37d35ed9"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super().__init__()\n",
        "\n",
        "        #TODO :  First lazy conv\n",
        "        self.conv1 = LazyConv2d_scratch(channels, kernel_size=3, padding=1, bias=False)\n",
        "        self.bn1 = BatchNorm(channels, num_dims=4)\n",
        "\n",
        "        #TODO  Second lazy conv\n",
        "        self.conv2 = LazyConv2d_scratch(channels, kernel_size=3, padding=1, bias=False)\n",
        "        self.bn2 = BatchNorm(channels, num_dims=4)\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        #TODO run first convolution, BN, ReLU\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = F.relu(out)\n",
        "\n",
        "       #TODO run second convolution, BN\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "\n",
        "        #TODO  skip connection, ReLU\n",
        "        out += identity\n",
        "        return F.relu(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e5bc270",
      "metadata": {
        "origin_pos": 6,
        "id": "2e5bc270"
      },
      "source": [
        "## LeNet\n",
        "\n",
        "At a high level, (**LeNet (LeNet-5) consists of two parts:\n",
        "(i) a convolutional encoder consisting of two convolutional layers; and\n",
        "(ii) a dense block consisting of three fully connected layers**).\n",
        "The architecture is summarized in :numref:`img_lenet`.\n",
        "\n",
        "![Data flow in LeNet. The input is a handwritten digit, the output is a probability over 10 possible outcomes.](../img/lenet.svg)\n",
        ":label:`img_lenet`\n",
        "\n",
        "The basic units in each convolutional block\n",
        "are a convolutional layer, a sigmoid activation function,\n",
        "and a subsequent average pooling operation.\n",
        "Note that while ReLUs and max-pooling work better,\n",
        "they had not yet been discovered.\n",
        "Each convolutional layer uses a $5\\times 5$ kernel\n",
        "and a sigmoid activation function.\n",
        "These layers map spatially arranged inputs\n",
        "to a number of two-dimensional feature maps, typically\n",
        "increasing the number of channels.\n",
        "The first convolutional layer has 6 output channels,\n",
        "while the second has 16.\n",
        "Each $2\\times2$ pooling operation (stride 2)\n",
        "reduces dimensionality by a factor of $4$ via spatial downsampling.\n",
        "The convolutional block emits an output with shape given by\n",
        "(batch size, number of channel, height, width).\n",
        "\n",
        "In order to pass output from the convolutional block\n",
        "to the dense block,\n",
        "we must flatten each example in the minibatch.\n",
        "In other words, we take this four-dimensional input and transform it\n",
        "into the two-dimensional input expected by fully connected layers:\n",
        "as a reminder, the two-dimensional representation that we desire uses the first dimension to index examples in the minibatch\n",
        "and the second to give the flat vector representation of each example.\n",
        "LeNet's dense block has three fully connected layers,\n",
        "with 120, 84, and 10 outputs, respectively.\n",
        "Because we are still performing classification,\n",
        "the 10-dimensional output layer corresponds\n",
        "to the number of possible output classes.\n",
        "\n",
        "While getting to the point where you truly understand\n",
        "what is going on inside LeNet may have taken a bit of work,\n",
        "we hope that the following code snippet will convince you\n",
        "that implementing such models with modern deep learning frameworks\n",
        "is remarkably simple.\n",
        "We need only to instantiate a `Sequential` block\n",
        "and chain together the appropriate layers,\n",
        "using Xavier initialization as\n",
        "introduced in :numref:`subsec_xavier`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5cd242a5",
      "metadata": {
        "origin_pos": 7,
        "tab": [
          "pytorch"
        ],
        "id": "5cd242a5"
      },
      "outputs": [],
      "source": [
        "def init_cnn(module):  #@save\n",
        "    \"\"\"Initialize weights for CNNs.\"\"\"\n",
        "    if type(module) == nn.Linear or type(module) == nn.Conv2d or type(module) == LazyConv2d_scratch :\n",
        "        nn.init.xavier_uniform_(module.weight)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66160314",
      "metadata": {
        "id": "66160314"
      },
      "outputs": [],
      "source": [
        "class LeNet(d2l.Classifier):  #@save\n",
        "    \"\"\"The LeNet-5 model.\"\"\"\n",
        "    def __init__(self, lr=0.1, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.LazyConv2d(6, kernel_size=5, padding=2), nn.Sigmoid(),\n",
        "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
        "            nn.LazyConv2d(16, kernel_size=5), nn.Sigmoid(),\n",
        "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            nn.Flatten(),\n",
        "            nn.LazyLinear(120), nn.Sigmoid(),\n",
        "            nn.LazyLinear(84), nn.Sigmoid(),\n",
        "            nn.LazyLinear(num_classes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "304add0f",
      "metadata": {
        "id": "304add0f"
      },
      "outputs": [],
      "source": [
        "class LeNet_scratch(d2l.Classifier):  #@save\n",
        "    \"\"\"The LeNet-5 model.\"\"\"\n",
        "    def __init__(self, lr=0.1, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.net = nn.Sequential(\n",
        "            LazyConv2d_scratch(6, kernel_size=5, padding=2), nn.Sigmoid(),\n",
        "            AvgPool2D_scratch(kernel_size=2, stride=2),\n",
        "            LazyConv2d_scratch(16, kernel_size=5), nn.Sigmoid(),\n",
        "            AvgPool2D_scratch(kernel_size=2, stride=2),\n",
        "            nn.Flatten(),\n",
        "            nn.LazyLinear(120), nn.Sigmoid(),\n",
        "            nn.LazyLinear(84), nn.Sigmoid(),\n",
        "            nn.LazyLinear(num_classes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1efa4c7",
      "metadata": {
        "id": "e1efa4c7"
      },
      "outputs": [],
      "source": [
        "class LeNet_scratch_ReLU(d2l.Classifier):  #@save\n",
        "    \"\"\"The LeNet-5 model.\"\"\"\n",
        "    def __init__(self, lr=0.1, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.net = nn.Sequential(\n",
        "            LazyConv2d_scratch( 6, kernel_size=5, padding=2), nn.ReLU(), AvgPool2D_scratch(kernel_size=2, stride=2),\n",
        "            LazyConv2d_scratch(16, kernel_size=5, padding=2), nn.ReLU(), AvgPool2D_scratch(kernel_size=2, stride=2),\n",
        "\n",
        "            nn.Flatten(),\n",
        "            nn.LazyLinear(120), nn.RelU(),\n",
        "            nn.LazyLinear(84), nn.RelU(),\n",
        "            nn.LazyLinear(num_classes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1915138c",
      "metadata": {
        "id": "1915138c"
      },
      "outputs": [],
      "source": [
        "class LeNet_scratch_ReLU_MaxPool(d2l.Classifier):  #@save\n",
        "    \"\"\"The LeNet-5 model.\"\"\"\n",
        "    def __init__(self, lr=0.1, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.net = nn.Sequential(\n",
        "            LazyConv2d_scratch(6, kernel_size=5, padding=2), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            LazyConv2d_scratch(16, kernel_size=5, padding=2), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "\n",
        "            nn.Flatten(),\n",
        "            nn.LazyLinear(120), nn.ReLU(),\n",
        "            nn.LazyLinear(84), nn.ReLU(),\n",
        "            nn.LazyLinear(num_classes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b1d2646",
      "metadata": {
        "id": "4b1d2646"
      },
      "outputs": [],
      "source": [
        "class LeNet_scratch_ReLU_MaxPool_3Conv(d2l.Classifier):  #@save\n",
        "    \"\"\"The LeNet-5 model.\"\"\"\n",
        "    def __init__(self, lr=0.1, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.net = nn.Sequential(\n",
        "            LazyConv2d_scratch(16, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            LazyConv2d_scratch(32, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            LazyConv2d_scratch(64, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "\n",
        "            nn.Flatten(),\n",
        "            nn.LazyLinear(120), nn.ReLU(),\n",
        "            nn.LazyLinear(84), nn.ReLU(),\n",
        "            nn.LazyLinear(num_classes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20dc7869",
      "metadata": {
        "origin_pos": 8,
        "tab": [
          "pytorch"
        ],
        "id": "20dc7869"
      },
      "outputs": [],
      "source": [
        "class BNLeNet(d2l.Classifier):\n",
        "    def __init__(self, lr=0.1, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.net = nn.Sequential(\n",
        "            LazyConv2d_scratch(16, kernel_size=3, padding=1, bias=False),\n",
        "            BatchNorm(16, num_dims=4),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "\n",
        "            LazyConv2d_scratch(32, kernel_size=3, padding=1, bias=False),\n",
        "            BatchNorm(32, num_dims=4),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "\n",
        "            LazyConv2d_scratch(64, kernel_size=3, padding=1, bias=False),\n",
        "            BatchNorm(64, num_dims=4),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "\n",
        "            nn.Flatten(),\n",
        "            nn.LazyLinear(120, bias=False), BatchNorm(120, num_dims=2), nn.ReLU(),\n",
        "            nn.LazyLinear(84,  bias=False), BatchNorm(84,  num_dims=2), nn.ReLU(),\n",
        "            nn.LazyLinear(num_classes)\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "882561b1",
      "metadata": {
        "id": "882561b1"
      },
      "outputs": [],
      "source": [
        "class BNLeNet_ResBlock(d2l.Classifier):\n",
        "    def __init__(self, lr=0.1, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.net = nn.Sequential(\n",
        "            # --- Block 1: 16 channels ---\n",
        "            LazyConv2d_scratch(16, kernel_size=3, padding=1, bias=False),\n",
        "            BatchNorm(16, num_dims=4),\n",
        "            nn.ReLU(),\n",
        "            ResidualBlock(16),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "\n",
        "            # --- Block 2: 32 channels ---\n",
        "            LazyConv2d_scratch(32, kernel_size=3, padding=1, bias=False),\n",
        "            BatchNorm(32, num_dims=4),\n",
        "            nn.ReLU(),\n",
        "            ResidualBlock(32),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "\n",
        "            # --- Block 3: 64 channels ---\n",
        "            LazyConv2d_scratch(64, kernel_size=3, padding=1, bias=False),\n",
        "            BatchNorm(64, num_dims=4),\n",
        "            nn.ReLU(),\n",
        "            ResidualBlock(64),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "\n",
        "            # --- Classifier head (unchanged) ---\n",
        "            nn.Flatten(),\n",
        "            nn.LazyLinear(120, bias=False),\n",
        "            BatchNorm(120, num_dims=2),\n",
        "            nn.ReLU(),\n",
        "            nn.LazyLinear(84,  bias=False),\n",
        "            BatchNorm(84,  num_dims=2),\n",
        "            nn.ReLU(),\n",
        "            nn.LazyLinear(num_classes)\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "084d8710",
      "metadata": {
        "origin_pos": 10,
        "id": "084d8710"
      },
      "source": [
        "We have taken some liberty in the reproduction of LeNet insofar as we have replaced the Gaussian activation layer by\n",
        "a softmax layer. This greatly simplifies the implementation, not least due to the\n",
        "fact that the Gaussian decoder is rarely used nowadays. Other than that, this network matches\n",
        "the original LeNet-5 architecture.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54aac677",
      "metadata": {
        "origin_pos": 11,
        "tab": [
          "pytorch"
        ],
        "id": "54aac677"
      },
      "source": [
        "Let's see what happens inside the network. By passing a\n",
        "single-channel (black and white)\n",
        "$28 \\times 28$ image through the network\n",
        "and printing the output shape at each layer,\n",
        "we can [**inspect the model**] to ensure\n",
        "that its operations line up with\n",
        "what we expect from :numref:`img_lenet_vert`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e7befc6",
      "metadata": {
        "origin_pos": 13,
        "id": "9e7befc6"
      },
      "source": [
        "![Compressed notation for LeNet-5.](../img/lenet-vert.svg)\n",
        ":label:`img_lenet_vert`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd006759",
      "metadata": {
        "origin_pos": 18,
        "tab": [
          "pytorch"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cd006759",
        "outputId": "dc1e38f5-cdf6-4af6-cc08-56730e535103"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 16.9MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 459kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 4.27MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 7.58MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LazyConv2d_scratch output shape:\t torch.Size([1, 16, 32, 32])\n",
            "BatchNorm output shape:\t torch.Size([1, 16, 32, 32])\n",
            "ReLU output shape:\t torch.Size([1, 16, 32, 32])\n",
            "ResidualBlock output shape:\t torch.Size([1, 16, 32, 32])\n",
            "MaxPool2d output shape:\t torch.Size([1, 16, 16, 16])\n",
            "LazyConv2d_scratch output shape:\t torch.Size([1, 32, 16, 16])\n",
            "BatchNorm output shape:\t torch.Size([1, 32, 16, 16])\n",
            "ReLU output shape:\t torch.Size([1, 32, 16, 16])\n",
            "ResidualBlock output shape:\t torch.Size([1, 32, 16, 16])\n",
            "MaxPool2d output shape:\t torch.Size([1, 32, 8, 8])\n",
            "LazyConv2d_scratch output shape:\t torch.Size([1, 64, 8, 8])\n",
            "BatchNorm output shape:\t torch.Size([1, 64, 8, 8])\n",
            "ReLU output shape:\t torch.Size([1, 64, 8, 8])\n",
            "ResidualBlock output shape:\t torch.Size([1, 64, 8, 8])\n",
            "MaxPool2d output shape:\t torch.Size([1, 64, 4, 4])\n",
            "Flatten output shape:\t torch.Size([1, 1024])\n",
            "Linear output shape:\t torch.Size([1, 120])\n",
            "BatchNorm output shape:\t torch.Size([1, 120])\n",
            "ReLU output shape:\t torch.Size([1, 120])\n",
            "Linear output shape:\t torch.Size([1, 84])\n",
            "BatchNorm output shape:\t torch.Size([1, 84])\n",
            "ReLU output shape:\t torch.Size([1, 84])\n",
            "Linear output shape:\t torch.Size([1, 10])\n"
          ]
        }
      ],
      "source": [
        "@d2l.add_to_class(d2l.Classifier)  #@save\n",
        "def layer_summary(self, X_shape):\n",
        "    X = torch.randn(*X_shape)\n",
        "    for layer in self.net:\n",
        "        X = layer(X)\n",
        "        print(layer.__class__.__name__, 'output shape:\\t', X.shape)\n",
        "\n",
        "# model = LeNet(lr=0.1)\n",
        "# model = LeNet_scratch(lr=0.1)\n",
        "# model = LeNet_scratch_ReLU_MaxPool(lr=0.2)\n",
        "# model = LeNet_scratch_ReLU_MaxPool_3Conv(lr=0.2)\n",
        "# model = BNLeNet(lr=0.2)\n",
        "model = BNLeNet_ResBlock(lr=0.3)\n",
        "\n",
        "data = MNISTData(batch_size=128)\n",
        "# data = CIFAR10Data(batch_size=256)\n",
        "#data = d2l.FashionMNIST(batch_size=128)\n",
        "\n",
        "model.apply_init([next(iter(data.get_dataloader(True)))[0]], init_cnn)\n",
        "train_iter = data.get_dataloader(True)\n",
        "test_iter  = data.get_dataloader(False)\n",
        "\n",
        "X, y = next(iter(train_iter))\n",
        "in_channels = X.shape[1]\n",
        "model.layer_summary((1, in_channels, X.shape[2], X.shape[3]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "568362d9",
      "metadata": {
        "id": "568362d9"
      },
      "source": [
        "### Basic LeNet on CIFAR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f82d79c",
      "metadata": {
        "origin_pos": 18,
        "tab": [
          "pytorch"
        ],
        "id": "8f82d79c",
        "outputId": "5bcb6927-744a-4588-908f-9ba05fe2bf31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch  1 | loss 2.3015 | train acc 0.1169 | test acc 0.1930\n",
            "epoch  2 | loss 2.0345 | train acc 0.2360 | test acc 0.2891\n",
            "epoch  3 | loss 1.9292 | train acc 0.2911 | test acc 0.3365\n",
            "epoch  4 | loss 1.8090 | train acc 0.3344 | test acc 0.3940\n",
            "epoch  5 | loss 1.7138 | train acc 0.3647 | test acc 0.4080\n",
            "epoch  6 | loss 1.6442 | train acc 0.3932 | test acc 0.4211\n",
            "epoch  7 | loss 1.6005 | train acc 0.4093 | test acc 0.4421\n",
            "epoch  8 | loss 1.5596 | train acc 0.4251 | test acc 0.4681\n",
            "epoch  9 | loss 1.5308 | train acc 0.4385 | test acc 0.4711\n",
            "epoch 10 | loss 1.5005 | train acc 0.4479 | test acc 0.4805\n",
            "epoch 11 | loss 1.4793 | train acc 0.4572 | test acc 0.4985\n",
            "epoch 12 | loss 1.4452 | train acc 0.4703 | test acc 0.5045\n",
            "epoch 13 | loss 1.4305 | train acc 0.4756 | test acc 0.5233\n",
            "epoch 14 | loss 1.4099 | train acc 0.4821 | test acc 0.5276\n",
            "epoch 15 | loss 1.3889 | train acc 0.4938 | test acc 0.5381\n",
            "epoch 16 | loss 1.3742 | train acc 0.5000 | test acc 0.5137\n",
            "epoch 17 | loss 1.3491 | train acc 0.5090 | test acc 0.5405\n",
            "epoch 18 | loss 1.3265 | train acc 0.5174 | test acc 0.5493\n",
            "epoch 19 | loss 1.3326 | train acc 0.5162 | test acc 0.5427\n",
            "epoch 20 | loss 1.3126 | train acc 0.5230 | test acc 0.5560\n",
            "epoch 21 | loss 1.2961 | train acc 0.5282 | test acc 0.5618\n",
            "epoch 22 | loss 1.2827 | train acc 0.5356 | test acc 0.5705\n",
            "epoch 23 | loss 1.2668 | train acc 0.5400 | test acc 0.5513\n",
            "epoch 24 | loss 1.2571 | train acc 0.5439 | test acc 0.5719\n",
            "epoch 25 | loss 1.2509 | train acc 0.5477 | test acc 0.5760\n"
          ]
        }
      ],
      "source": [
        "# Note that the height and width of the representation\n",
        "# at each layer throughout the convolutional block\n",
        "# is reduced (compared with the previous layer).\n",
        "# The first convolutional layer uses two pixels of padding\n",
        "# to compensate for the reduction in height and width\n",
        "# that would otherwise result from using a $5 \\times 5$ kernel.\n",
        "# As an aside, the image size of $28 \\times 28$ pixels in the original\n",
        "# MNIST OCR dataset is a result of *trimming* two pixel rows (and columns) from the\n",
        "# original scans that measured $32 \\times 32$ pixels. This was done primarily to\n",
        "# save space (a 30% reduction) at a time when megabytes mattered.\n",
        "\n",
        "# In contrast, the second convolutional layer forgoes padding,\n",
        "# and thus the height and width are both reduced by four pixels.\n",
        "# As we go up the stack of layers,\n",
        "# the number of channels increases layer-over-layer\n",
        "# from 1 in the input to 6 after the first convolutional layer\n",
        "# and 16 after the second convolutional layer.\n",
        "# However, each pooling layer halves the height and width.\n",
        "# Finally, each fully connected layer reduces dimensionality,\n",
        "# finally emitting an output whose dimension\n",
        "# matches the number of classes.\n",
        "\n",
        "\n",
        "## Training\n",
        "\n",
        "# Now that we have implemented the model,\n",
        "# let's [**run an experiment to see how the LeNet-5 model fares on Fashion-MNIST**].\n",
        "\n",
        "# While CNNs have fewer parameters,\n",
        "# they can still be more expensive to compute\n",
        "# than similarly deep MLPs\n",
        "# because each parameter participates in many more\n",
        "# multiplications.\n",
        "# If you have access to a GPU, this might be a good time\n",
        "# to put it into action to speed up training.\n",
        "# Note that\n",
        "# the `d2l.Trainer` class takes care of all details.\n",
        "# By default, it initializes the model parameters on the\n",
        "# available devices.\n",
        "# Just as with MLPs, our loss function is cross-entropy,\n",
        "# and we minimize it via minibatch stochastic gradient descent.\n",
        "\n",
        "\n",
        "trainer = d2l.Trainer(max_epochs=30, num_gpus=1)\n",
        "trainer.show_progress = True\n",
        "\n",
        "\n",
        "\n",
        "device = d2l.try_gpu()\n",
        "model = model.to(device)\n",
        "\n",
        "\n",
        "lr = getattr(model, \"lr\", 0.1)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "num_epochs = 25\n",
        "\n",
        "# --- buffers (no board yet) ---\n",
        "epochs_hist, train_loss_hist, train_acc_hist, test_acc_hist = [], [], [], []\n",
        "\n",
        "def evaluate_accuracy(model, loader, device):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in loader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X).argmax(1)\n",
        "            correct += (pred == y).sum().item()\n",
        "            total   += y.numel()\n",
        "    return correct / max(total, 1)\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    model.train()\n",
        "    sum_loss = 0.0\n",
        "    correct, total = 0, 0\n",
        "\n",
        "    for X, y in train_iter:\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        logits = model(X)\n",
        "        loss = loss_fn(logits, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        bs = y.numel()\n",
        "        sum_loss += loss.item() * bs\n",
        "        correct  += (logits.argmax(1) == y).sum().item()\n",
        "        total    += bs\n",
        "\n",
        "    train_loss = sum_loss / total\n",
        "    train_acc  = correct / total\n",
        "    test_acc   = evaluate_accuracy(model, test_iter, device)\n",
        "\n",
        "    # print now\n",
        "    print(f\"epoch {epoch:2d} | loss {train_loss:.4f} | \"\n",
        "          f\"train acc {train_acc:.4f} | test acc {test_acc:.4f}\", flush=True)\n",
        "\n",
        "    # buffer for plotting later\n",
        "    epochs_hist.append(epoch)\n",
        "    train_loss_hist.append(train_loss)\n",
        "    train_acc_hist.append(train_acc)\n",
        "    test_acc_hist.append(test_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b95b48e5",
      "metadata": {
        "id": "b95b48e5"
      },
      "source": [
        "### Basic LeNet on MNIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54afde09",
      "metadata": {
        "id": "54afde09",
        "outputId": "3dd63f32-0d6b-4866-8ffd-09fcb3aee2e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch  1 | loss 2.3115 | train acc 0.1071 | test acc 0.1135\n",
            "epoch  2 | loss 1.7743 | train acc 0.3472 | test acc 0.6865\n",
            "epoch  3 | loss 0.8243 | train acc 0.7184 | test acc 0.8373\n",
            "epoch  4 | loss 0.5045 | train acc 0.8315 | test acc 0.8851\n",
            "epoch  5 | loss 0.3827 | train acc 0.8725 | test acc 0.9138\n",
            "epoch  6 | loss 0.3208 | train acc 0.8929 | test acc 0.9293\n",
            "epoch  7 | loss 0.2690 | train acc 0.9126 | test acc 0.9364\n",
            "epoch  8 | loss 0.2450 | train acc 0.9218 | test acc 0.9447\n",
            "epoch  9 | loss 0.2204 | train acc 0.9304 | test acc 0.9559\n",
            "epoch 10 | loss 0.2011 | train acc 0.9352 | test acc 0.9398\n",
            "epoch 11 | loss 0.1847 | train acc 0.9414 | test acc 0.9569\n",
            "epoch 12 | loss 0.1723 | train acc 0.9445 | test acc 0.9636\n",
            "epoch 13 | loss 0.1629 | train acc 0.9481 | test acc 0.9569\n",
            "epoch 14 | loss 0.1609 | train acc 0.9491 | test acc 0.9661\n",
            "epoch 15 | loss 0.1513 | train acc 0.9520 | test acc 0.9642\n",
            "epoch 16 | loss 0.1453 | train acc 0.9535 | test acc 0.9617\n",
            "epoch 17 | loss 0.1336 | train acc 0.9570 | test acc 0.9696\n",
            "epoch 18 | loss 0.1342 | train acc 0.9581 | test acc 0.9684\n",
            "epoch 19 | loss 0.1276 | train acc 0.9592 | test acc 0.9675\n",
            "epoch 20 | loss 0.1249 | train acc 0.9602 | test acc 0.9693\n",
            "epoch 21 | loss 0.1192 | train acc 0.9613 | test acc 0.9745\n",
            "epoch 22 | loss 0.1183 | train acc 0.9627 | test acc 0.9730\n",
            "epoch 23 | loss 0.1112 | train acc 0.9644 | test acc 0.9680\n",
            "epoch 24 | loss 0.1102 | train acc 0.9648 | test acc 0.9758\n",
            "epoch 25 | loss 0.1079 | train acc 0.9653 | test acc 0.9759\n"
          ]
        }
      ],
      "source": [
        "trainer = d2l.Trainer(max_epochs=30, num_gpus=1)\n",
        "trainer.show_progress = True\n",
        "\n",
        "\n",
        "\n",
        "device = d2l.try_gpu()\n",
        "model = model.to(device)\n",
        "\n",
        "\n",
        "lr = getattr(model, \"lr\", 0.1)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "num_epochs = 25\n",
        "\n",
        "# --- buffers (no board yet) ---\n",
        "epochs_hist, train_loss_hist, train_acc_hist, test_acc_hist = [], [], [], []\n",
        "\n",
        "def evaluate_accuracy(model, loader, device):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in loader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X).argmax(1)\n",
        "            correct += (pred == y).sum().item()\n",
        "            total   += y.numel()\n",
        "    return correct / max(total, 1)\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    model.train()\n",
        "    sum_loss = 0.0\n",
        "    correct, total = 0, 0\n",
        "\n",
        "    for X, y in train_iter:\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        logits = model(X)\n",
        "        loss = loss_fn(logits, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        bs = y.numel()\n",
        "        sum_loss += loss.item() * bs\n",
        "        correct  += (logits.argmax(1) == y).sum().item()\n",
        "        total    += bs\n",
        "\n",
        "    train_loss = sum_loss / total\n",
        "    train_acc  = correct / total\n",
        "    test_acc   = evaluate_accuracy(model, test_iter, device)\n",
        "\n",
        "    # print now\n",
        "    print(f\"epoch {epoch:2d} | loss {train_loss:.4f} | \"\n",
        "          f\"train acc {train_acc:.4f} | test acc {test_acc:.4f}\", flush=True)\n",
        "\n",
        "    # buffer for plotting later\n",
        "    epochs_hist.append(epoch)\n",
        "    train_loss_hist.append(train_loss)\n",
        "    train_acc_hist.append(train_acc)\n",
        "    test_acc_hist.append(test_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11a7dd83",
      "metadata": {
        "id": "11a7dd83"
      },
      "source": [
        "### LeNet Scratch on CIFAR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3d55e1f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3d55e1f",
        "outputId": "e691d322-d757-4c37-f419-ded2396527ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch  1 | loss 2.2452 | train acc 0.1382 | test acc 0.2091\n",
            "epoch  2 | loss 2.0142 | train acc 0.2453 | test acc 0.3108\n",
            "epoch  3 | loss 1.9100 | train acc 0.2949 | test acc 0.3572\n",
            "epoch  4 | loss 1.8061 | train acc 0.3346 | test acc 0.3777\n",
            "epoch  5 | loss 1.7255 | train acc 0.3644 | test acc 0.4124\n",
            "epoch  6 | loss 1.6646 | train acc 0.3873 | test acc 0.4284\n",
            "epoch  7 | loss 1.6156 | train acc 0.4066 | test acc 0.4552\n",
            "epoch  8 | loss 1.5769 | train acc 0.4219 | test acc 0.4576\n",
            "epoch  9 | loss 1.5530 | train acc 0.4320 | test acc 0.4585\n",
            "epoch 10 | loss 1.5112 | train acc 0.4467 | test acc 0.4830\n",
            "epoch 11 | loss 1.4895 | train acc 0.4536 | test acc 0.4949\n",
            "epoch 12 | loss 1.4638 | train acc 0.4641 | test acc 0.4957\n",
            "epoch 13 | loss 1.4446 | train acc 0.4720 | test acc 0.5115\n",
            "epoch 14 | loss 1.4179 | train acc 0.4837 | test acc 0.5211\n",
            "epoch 15 | loss 1.4048 | train acc 0.4901 | test acc 0.5297\n",
            "epoch 16 | loss 1.3868 | train acc 0.4960 | test acc 0.5281\n",
            "epoch 17 | loss 1.3659 | train acc 0.5053 | test acc 0.5321\n",
            "epoch 18 | loss 1.3480 | train acc 0.5103 | test acc 0.5518\n",
            "epoch 19 | loss 1.3418 | train acc 0.5163 | test acc 0.5572\n",
            "epoch 20 | loss 1.3299 | train acc 0.5195 | test acc 0.5618\n",
            "epoch 21 | loss 1.3143 | train acc 0.5250 | test acc 0.5611\n",
            "epoch 22 | loss 1.2811 | train acc 0.5365 | test acc 0.5656\n",
            "epoch 23 | loss 1.2688 | train acc 0.5425 | test acc 0.5695\n",
            "epoch 24 | loss 1.2609 | train acc 0.5449 | test acc 0.5777\n",
            "epoch 25 | loss 1.2417 | train acc 0.5498 | test acc 0.5736\n"
          ]
        }
      ],
      "source": [
        "# Note that the height and width of the representation\n",
        "# at each layer throughout the convolutional block\n",
        "# is reduced (compared with the previous layer).\n",
        "# The first convolutional layer uses two pixels of padding\n",
        "# to compensate for the reduction in height and width\n",
        "# that would otherwise result from using a $5 \\times 5$ kernel.\n",
        "# As an aside, the image size of $28 \\times 28$ pixels in the original\n",
        "# MNIST OCR dataset is a result of *trimming* two pixel rows (and columns) from the\n",
        "# original scans that measured $32 \\times 32$ pixels. This was done primarily to\n",
        "# save space (a 30% reduction) at a time when megabytes mattered.\n",
        "\n",
        "# In contrast, the second convolutional layer forgoes padding,\n",
        "# and thus the height and width are both reduced by four pixels.\n",
        "# As we go up the stack of layers,\n",
        "# the number of channels increases layer-over-layer\n",
        "# from 1 in the input to 6 after the first convolutional layer\n",
        "# and 16 after the second convolutional layer.\n",
        "# However, each pooling layer halves the height and width.\n",
        "# Finally, each fully connected layer reduces dimensionality,\n",
        "# finally emitting an output whose dimension\n",
        "# matches the number of classes.\n",
        "\n",
        "\n",
        "## Training\n",
        "\n",
        "# Now that we have implemented the model,\n",
        "# let's [**run an experiment to see how the LeNet-5 model fares on Fashion-MNIST**].\n",
        "\n",
        "# While CNNs have fewer parameters,\n",
        "# they can still be more expensive to compute\n",
        "# than similarly deep MLPs\n",
        "# because each parameter participates in many more\n",
        "# multiplications.\n",
        "# If you have access to a GPU, this might be a good time\n",
        "# to put it into action to speed up training.\n",
        "# Note that\n",
        "# the `d2l.Trainer` class takes care of all details.\n",
        "# By default, it initializes the model parameters on the\n",
        "# available devices.\n",
        "# Just as with MLPs, our loss function is cross-entropy,\n",
        "# and we minimize it via minibatch stochastic gradient descent.\n",
        "\n",
        "\n",
        "trainer = d2l.Trainer(max_epochs=30, num_gpus=1)\n",
        "trainer.show_progress = True\n",
        "\n",
        "\n",
        "\n",
        "device = d2l.try_gpu()\n",
        "model = model.to(device)\n",
        "\n",
        "\n",
        "lr = getattr(model, \"lr\", 0.1)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "num_epochs = 25\n",
        "\n",
        "# --- buffers (no board yet) ---\n",
        "epochs_hist, train_loss_hist, train_acc_hist, test_acc_hist = [], [], [], []\n",
        "\n",
        "def evaluate_accuracy(model, loader, device):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in loader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X).argmax(1)\n",
        "            correct += (pred == y).sum().item()\n",
        "            total   += y.numel()\n",
        "    return correct / max(total, 1)\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    model.train()\n",
        "    sum_loss = 0.0\n",
        "    correct, total = 0, 0\n",
        "\n",
        "    for X, y in train_iter:\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        logits = model(X)\n",
        "        loss = loss_fn(logits, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        bs = y.numel()\n",
        "        sum_loss += loss.item() * bs\n",
        "        correct  += (logits.argmax(1) == y).sum().item()\n",
        "        total    += bs\n",
        "\n",
        "    train_loss = sum_loss / total\n",
        "    train_acc  = correct / total\n",
        "    test_acc   = evaluate_accuracy(model, test_iter, device)\n",
        "\n",
        "    # print now\n",
        "    print(f\"epoch {epoch:2d} | loss {train_loss:.4f} | \"\n",
        "          f\"train acc {train_acc:.4f} | test acc {test_acc:.4f}\", flush=True)\n",
        "\n",
        "    # buffer for plotting later\n",
        "    epochs_hist.append(epoch)\n",
        "    train_loss_hist.append(train_loss)\n",
        "    train_acc_hist.append(train_acc)\n",
        "    test_acc_hist.append(test_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ad96f95",
      "metadata": {
        "id": "9ad96f95"
      },
      "source": [
        "### Lenet Scratch on MNIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62c22f80",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62c22f80",
        "outputId": "bf4bda7d-0992-4272-afb2-081cc10d2f36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch  1 | loss 2.1678 | train acc 0.1841 | test acc 0.5305\n",
            "epoch  2 | loss 1.1170 | train acc 0.6072 | test acc 0.7715\n",
            "epoch  3 | loss 0.6207 | train acc 0.7887 | test acc 0.8358\n",
            "epoch  4 | loss 0.4373 | train acc 0.8520 | test acc 0.9016\n",
            "epoch  5 | loss 0.3500 | train acc 0.8807 | test acc 0.9150\n",
            "epoch  6 | loss 0.2911 | train acc 0.9042 | test acc 0.9369\n",
            "epoch  7 | loss 0.2473 | train acc 0.9185 | test acc 0.9371\n",
            "epoch  8 | loss 0.2169 | train acc 0.9297 | test acc 0.9516\n",
            "epoch  9 | loss 0.1964 | train acc 0.9371 | test acc 0.9556\n",
            "epoch 10 | loss 0.1769 | train acc 0.9440 | test acc 0.9646\n",
            "epoch 11 | loss 0.1617 | train acc 0.9479 | test acc 0.9586\n",
            "epoch 12 | loss 0.1547 | train acc 0.9503 | test acc 0.9633\n",
            "epoch 13 | loss 0.1440 | train acc 0.9546 | test acc 0.9603\n",
            "epoch 14 | loss 0.1403 | train acc 0.9553 | test acc 0.9683\n",
            "epoch 15 | loss 0.1284 | train acc 0.9594 | test acc 0.9709\n",
            "epoch 16 | loss 0.1254 | train acc 0.9606 | test acc 0.9717\n",
            "epoch 17 | loss 0.1221 | train acc 0.9612 | test acc 0.9691\n",
            "epoch 18 | loss 0.1212 | train acc 0.9614 | test acc 0.9720\n",
            "epoch 19 | loss 0.1105 | train acc 0.9648 | test acc 0.9769\n",
            "epoch 20 | loss 0.1114 | train acc 0.9650 | test acc 0.9730\n",
            "epoch 21 | loss 0.1064 | train acc 0.9661 | test acc 0.9739\n",
            "epoch 22 | loss 0.1046 | train acc 0.9663 | test acc 0.9727\n",
            "epoch 23 | loss 0.1029 | train acc 0.9668 | test acc 0.9734\n",
            "epoch 24 | loss 0.0983 | train acc 0.9681 | test acc 0.9768\n",
            "epoch 25 | loss 0.0970 | train acc 0.9689 | test acc 0.9755\n"
          ]
        }
      ],
      "source": [
        "trainer = d2l.Trainer(max_epochs=30, num_gpus=1)\n",
        "trainer.show_progress = True\n",
        "\n",
        "\n",
        "\n",
        "device = d2l.try_gpu()\n",
        "model = model.to(device)\n",
        "\n",
        "\n",
        "lr = getattr(model, \"lr\", 0.1)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "num_epochs = 25\n",
        "\n",
        "# --- buffers (no board yet) ---\n",
        "epochs_hist, train_loss_hist, train_acc_hist, test_acc_hist = [], [], [], []\n",
        "\n",
        "def evaluate_accuracy(model, loader, device):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in loader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X).argmax(1)\n",
        "            correct += (pred == y).sum().item()\n",
        "            total   += y.numel()\n",
        "    return correct / max(total, 1)\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    model.train()\n",
        "    sum_loss = 0.0\n",
        "    correct, total = 0, 0\n",
        "\n",
        "    for X, y in train_iter:\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        logits = model(X)\n",
        "        loss = loss_fn(logits, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        bs = y.numel()\n",
        "        sum_loss += loss.item() * bs\n",
        "        correct  += (logits.argmax(1) == y).sum().item()\n",
        "        total    += bs\n",
        "\n",
        "    train_loss = sum_loss / total\n",
        "    train_acc  = correct / total\n",
        "    test_acc   = evaluate_accuracy(model, test_iter, device)\n",
        "\n",
        "    # print now\n",
        "    print(f\"epoch {epoch:2d} | loss {train_loss:.4f} | \"\n",
        "          f\"train acc {train_acc:.4f} | test acc {test_acc:.4f}\", flush=True)\n",
        "\n",
        "    # buffer for plotting later\n",
        "    epochs_hist.append(epoch)\n",
        "    train_loss_hist.append(train_loss)\n",
        "    train_acc_hist.append(train_acc)\n",
        "    test_acc_hist.append(test_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LeNet Relu MaxPool on CIFAR"
      ],
      "metadata": {
        "id": "8GrvBNX49MpO"
      },
      "id": "8GrvBNX49MpO"
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = d2l.Trainer(max_epochs=30, num_gpus=1)\n",
        "trainer.show_progress = True\n",
        "\n",
        "\n",
        "\n",
        "device = d2l.try_gpu()\n",
        "model = model.to(device)\n",
        "\n",
        "\n",
        "lr = getattr(model, \"lr\", 0.1)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "num_epochs = 25\n",
        "\n",
        "# --- buffers (no board yet) ---\n",
        "epochs_hist, train_loss_hist, train_acc_hist, test_acc_hist = [], [], [], []\n",
        "\n",
        "def evaluate_accuracy(model, loader, device):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in loader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X).argmax(1)\n",
        "            correct += (pred == y).sum().item()\n",
        "            total   += y.numel()\n",
        "    return correct / max(total, 1)\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    model.train()\n",
        "    sum_loss = 0.0\n",
        "    correct, total = 0, 0\n",
        "\n",
        "    for X, y in train_iter:\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        logits = model(X)\n",
        "        loss = loss_fn(logits, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        bs = y.numel()\n",
        "        sum_loss += loss.item() * bs\n",
        "        correct  += (logits.argmax(1) == y).sum().item()\n",
        "        total    += bs\n",
        "\n",
        "    train_loss = sum_loss / total\n",
        "    train_acc  = correct / total\n",
        "    test_acc   = evaluate_accuracy(model, test_iter, device)\n",
        "\n",
        "    # print now\n",
        "    print(f\"epoch {epoch:2d} | loss {train_loss:.4f} | \"\n",
        "          f\"train acc {train_acc:.4f} | test acc {test_acc:.4f}\", flush=True)\n",
        "\n",
        "    # buffer for plotting later\n",
        "    epochs_hist.append(epoch)\n",
        "    train_loss_hist.append(train_loss)\n",
        "    train_acc_hist.append(train_acc)\n",
        "    test_acc_hist.append(test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zYDq_SaX9Sqn",
        "outputId": "0457d2b6-17fa-438b-c206-c850a26cab2c"
      },
      "id": "zYDq_SaX9Sqn",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch  1 | loss 1.9580 | train acc 0.2640 | test acc 0.3279\n",
            "epoch  2 | loss 1.7336 | train acc 0.3557 | test acc 0.4344\n",
            "epoch  3 | loss 1.6134 | train acc 0.4118 | test acc 0.4609\n",
            "epoch  4 | loss 1.5605 | train acc 0.4372 | test acc 0.4648\n",
            "epoch  5 | loss 1.4906 | train acc 0.4651 | test acc 0.4423\n",
            "epoch  6 | loss 1.4957 | train acc 0.4652 | test acc 0.5252\n",
            "epoch  7 | loss 1.4390 | train acc 0.4866 | test acc 0.5390\n",
            "epoch  8 | loss 1.4128 | train acc 0.5031 | test acc 0.5438\n",
            "epoch  9 | loss 1.4047 | train acc 0.5047 | test acc 0.5233\n",
            "epoch 10 | loss 1.3915 | train acc 0.5142 | test acc 0.5111\n",
            "epoch 11 | loss 1.3674 | train acc 0.5216 | test acc 0.5567\n",
            "epoch 12 | loss 1.3676 | train acc 0.5267 | test acc 0.5472\n",
            "epoch 13 | loss 1.3491 | train acc 0.5316 | test acc 0.5475\n",
            "epoch 14 | loss 1.3188 | train acc 0.5433 | test acc 0.5626\n",
            "epoch 15 | loss 1.3490 | train acc 0.5338 | test acc 0.5677\n",
            "epoch 16 | loss 1.3323 | train acc 0.5452 | test acc 0.5764\n",
            "epoch 17 | loss 1.3208 | train acc 0.5471 | test acc 0.5731\n",
            "epoch 18 | loss 1.3081 | train acc 0.5519 | test acc 0.5825\n",
            "epoch 19 | loss 1.3107 | train acc 0.5517 | test acc 0.5987\n",
            "epoch 20 | loss 1.3237 | train acc 0.5492 | test acc 0.5636\n",
            "epoch 21 | loss 1.2983 | train acc 0.5581 | test acc 0.5881\n",
            "epoch 22 | loss 1.2894 | train acc 0.5607 | test acc 0.5943\n",
            "epoch 23 | loss 1.3001 | train acc 0.5573 | test acc 0.6060\n",
            "epoch 24 | loss 1.3091 | train acc 0.5573 | test acc 0.5887\n",
            "epoch 25 | loss 1.3038 | train acc 0.5605 | test acc 0.5870\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LeNet Relu MaxPool on MNIST"
      ],
      "metadata": {
        "id": "Y8clR1jU_fvH"
      },
      "id": "Y8clR1jU_fvH"
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = d2l.Trainer(max_epochs=30, num_gpus=1)\n",
        "trainer.show_progress = True\n",
        "\n",
        "\n",
        "\n",
        "device = d2l.try_gpu()\n",
        "model = model.to(device)\n",
        "\n",
        "\n",
        "lr = getattr(model, \"lr\", 0.1)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "num_epochs = 25\n",
        "\n",
        "# --- buffers (no board yet) ---\n",
        "epochs_hist, train_loss_hist, train_acc_hist, test_acc_hist = [], [], [], []\n",
        "\n",
        "def evaluate_accuracy(model, loader, device):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in loader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X).argmax(1)\n",
        "            correct += (pred == y).sum().item()\n",
        "            total   += y.numel()\n",
        "    return correct / max(total, 1)\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    model.train()\n",
        "    sum_loss = 0.0\n",
        "    correct, total = 0, 0\n",
        "\n",
        "    for X, y in train_iter:\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        logits = model(X)\n",
        "        loss = loss_fn(logits, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        bs = y.numel()\n",
        "        sum_loss += loss.item() * bs\n",
        "        correct  += (logits.argmax(1) == y).sum().item()\n",
        "        total    += bs\n",
        "\n",
        "    train_loss = sum_loss / total\n",
        "    train_acc  = correct / total\n",
        "    test_acc   = evaluate_accuracy(model, test_iter, device)\n",
        "\n",
        "    # print now\n",
        "    print(f\"epoch {epoch:2d} | loss {train_loss:.4f} | \"\n",
        "          f\"train acc {train_acc:.4f} | test acc {test_acc:.4f}\", flush=True)\n",
        "\n",
        "    # buffer for plotting later\n",
        "    epochs_hist.append(epoch)\n",
        "    train_loss_hist.append(train_loss)\n",
        "    train_acc_hist.append(train_acc)\n",
        "    test_acc_hist.append(test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WS8be8aj_eMW",
        "outputId": "602be871-e145-4d1f-cbff-195be2e8c41a"
      },
      "id": "WS8be8aj_eMW",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch  1 | loss 0.9022 | train acc 0.7044 | test acc 0.7917\n",
            "epoch  2 | loss 0.3843 | train acc 0.8860 | test acc 0.9289\n",
            "epoch  3 | loss 0.2984 | train acc 0.9143 | test acc 0.9529\n",
            "epoch  4 | loss 0.2598 | train acc 0.9280 | test acc 0.9297\n",
            "epoch  5 | loss 0.2356 | train acc 0.9335 | test acc 0.9451\n",
            "epoch  6 | loss 0.2256 | train acc 0.9372 | test acc 0.9495\n",
            "epoch  7 | loss 0.2185 | train acc 0.9404 | test acc 0.9575\n",
            "epoch  8 | loss 0.2195 | train acc 0.9409 | test acc 0.9597\n",
            "epoch  9 | loss 0.2116 | train acc 0.9433 | test acc 0.9437\n",
            "epoch 10 | loss 0.1994 | train acc 0.9466 | test acc 0.9631\n",
            "epoch 11 | loss 0.1959 | train acc 0.9478 | test acc 0.9588\n",
            "epoch 12 | loss 0.1936 | train acc 0.9483 | test acc 0.9470\n",
            "epoch 13 | loss 0.1898 | train acc 0.9502 | test acc 0.9645\n",
            "epoch 14 | loss 0.1837 | train acc 0.9517 | test acc 0.9602\n",
            "epoch 15 | loss 0.1866 | train acc 0.9512 | test acc 0.9625\n",
            "epoch 16 | loss 0.1855 | train acc 0.9516 | test acc 0.9634\n",
            "epoch 17 | loss 0.1814 | train acc 0.9523 | test acc 0.9609\n",
            "epoch 18 | loss 0.1782 | train acc 0.9525 | test acc 0.9551\n",
            "epoch 19 | loss 0.1659 | train acc 0.9575 | test acc 0.9681\n",
            "epoch 20 | loss 0.1641 | train acc 0.9580 | test acc 0.9552\n",
            "epoch 21 | loss 0.1729 | train acc 0.9552 | test acc 0.9559\n",
            "epoch 22 | loss 0.1906 | train acc 0.9522 | test acc 0.9568\n",
            "epoch 23 | loss 0.1752 | train acc 0.9547 | test acc 0.9686\n",
            "epoch 24 | loss 0.1847 | train acc 0.9534 | test acc 0.9637\n",
            "epoch 25 | loss 0.1617 | train acc 0.9572 | test acc 0.9597\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LeNet Relu MaxPool 3Conv on CIFAR"
      ],
      "metadata": {
        "id": "WiiXNLsFAB1l"
      },
      "id": "WiiXNLsFAB1l"
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = d2l.Trainer(max_epochs=30, num_gpus=1)\n",
        "trainer.show_progress = True\n",
        "\n",
        "\n",
        "\n",
        "device = d2l.try_gpu()\n",
        "model = model.to(device)\n",
        "\n",
        "\n",
        "lr = getattr(model, \"lr\", 0.1)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "num_epochs = 25\n",
        "\n",
        "# --- buffers (no board yet) ---\n",
        "epochs_hist, train_loss_hist, train_acc_hist, test_acc_hist = [], [], [], []\n",
        "\n",
        "def evaluate_accuracy(model, loader, device):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in loader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X).argmax(1)\n",
        "            correct += (pred == y).sum().item()\n",
        "            total   += y.numel()\n",
        "    return correct / max(total, 1)\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    model.train()\n",
        "    sum_loss = 0.0\n",
        "    correct, total = 0, 0\n",
        "\n",
        "    for X, y in train_iter:\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        logits = model(X)\n",
        "        loss = loss_fn(logits, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        bs = y.numel()\n",
        "        sum_loss += loss.item() * bs\n",
        "        correct  += (logits.argmax(1) == y).sum().item()\n",
        "        total    += bs\n",
        "\n",
        "    train_loss = sum_loss / total\n",
        "    train_acc  = correct / total\n",
        "    test_acc   = evaluate_accuracy(model, test_iter, device)\n",
        "\n",
        "    # print now\n",
        "    print(f\"epoch {epoch:2d} | loss {train_loss:.4f} | \"\n",
        "          f\"train acc {train_acc:.4f} | test acc {test_acc:.4f}\", flush=True)\n",
        "\n",
        "    # buffer for plotting later\n",
        "    epochs_hist.append(epoch)\n",
        "    train_loss_hist.append(train_loss)\n",
        "    train_acc_hist.append(train_acc)\n",
        "    test_acc_hist.append(test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KIj2_yH1ABhF",
        "outputId": "777f6b84-2a04-4276-a71f-364deffdee0c"
      },
      "id": "KIj2_yH1ABhF",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch  1 | loss 1.8205 | train acc 0.3283 | test acc 0.4385\n",
            "epoch  2 | loss 1.4730 | train acc 0.4623 | test acc 0.5006\n",
            "epoch  3 | loss 1.3050 | train acc 0.5338 | test acc 0.5751\n",
            "epoch  4 | loss 1.1957 | train acc 0.5785 | test acc 0.6012\n",
            "epoch  5 | loss 1.1546 | train acc 0.5938 | test acc 0.6415\n",
            "epoch  6 | loss 1.1006 | train acc 0.6165 | test acc 0.6379\n",
            "epoch  7 | loss 1.0767 | train acc 0.6259 | test acc 0.6125\n",
            "epoch  8 | loss 1.0594 | train acc 0.6319 | test acc 0.6510\n",
            "epoch  9 | loss 1.0311 | train acc 0.6456 | test acc 0.6762\n",
            "epoch 10 | loss 1.0092 | train acc 0.6533 | test acc 0.6676\n",
            "epoch 11 | loss 1.0007 | train acc 0.6566 | test acc 0.6438\n",
            "epoch 12 | loss 1.0110 | train acc 0.6534 | test acc 0.6708\n",
            "epoch 13 | loss 0.9970 | train acc 0.6623 | test acc 0.6769\n",
            "epoch 14 | loss 0.9892 | train acc 0.6646 | test acc 0.6907\n",
            "epoch 15 | loss 0.9865 | train acc 0.6669 | test acc 0.6781\n",
            "epoch 16 | loss 0.9778 | train acc 0.6720 | test acc 0.6529\n",
            "epoch 17 | loss 0.9493 | train acc 0.6802 | test acc 0.6872\n",
            "epoch 18 | loss 0.9620 | train acc 0.6786 | test acc 0.6880\n",
            "epoch 19 | loss 0.9485 | train acc 0.6832 | test acc 0.6932\n",
            "epoch 20 | loss 0.9509 | train acc 0.6829 | test acc 0.7013\n",
            "epoch 21 | loss 0.9426 | train acc 0.6866 | test acc 0.6868\n",
            "epoch 22 | loss 0.9453 | train acc 0.6877 | test acc 0.7020\n",
            "epoch 23 | loss 0.9455 | train acc 0.6876 | test acc 0.7042\n",
            "epoch 24 | loss 0.9367 | train acc 0.6914 | test acc 0.6973\n",
            "epoch 25 | loss 0.9442 | train acc 0.6884 | test acc 0.6961\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LeNet Relu MaxPool 3Conv on MNIST"
      ],
      "metadata": {
        "id": "qhNi6kkEAF74"
      },
      "id": "qhNi6kkEAF74"
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = d2l.Trainer(max_epochs=30, num_gpus=1)\n",
        "trainer.show_progress = True\n",
        "\n",
        "\n",
        "\n",
        "device = d2l.try_gpu()\n",
        "model = model.to(device)\n",
        "\n",
        "\n",
        "lr = getattr(model, \"lr\", 0.1)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "num_epochs = 25\n",
        "\n",
        "# --- buffers (no board yet) ---\n",
        "epochs_hist, train_loss_hist, train_acc_hist, test_acc_hist = [], [], [], []\n",
        "\n",
        "def evaluate_accuracy(model, loader, device):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in loader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X).argmax(1)\n",
        "            correct += (pred == y).sum().item()\n",
        "            total   += y.numel()\n",
        "    return correct / max(total, 1)\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    model.train()\n",
        "    sum_loss = 0.0\n",
        "    correct, total = 0, 0\n",
        "\n",
        "    for X, y in train_iter:\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        logits = model(X)\n",
        "        loss = loss_fn(logits, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        bs = y.numel()\n",
        "        sum_loss += loss.item() * bs\n",
        "        correct  += (logits.argmax(1) == y).sum().item()\n",
        "        total    += bs\n",
        "\n",
        "    train_loss = sum_loss / total\n",
        "    train_acc  = correct / total\n",
        "    test_acc   = evaluate_accuracy(model, test_iter, device)\n",
        "\n",
        "    # print now\n",
        "    print(f\"epoch {epoch:2d} | loss {train_loss:.4f} | \"\n",
        "          f\"train acc {train_acc:.4f} | test acc {test_acc:.4f}\", flush=True)\n",
        "\n",
        "    # buffer for plotting later\n",
        "    epochs_hist.append(epoch)\n",
        "    train_loss_hist.append(train_loss)\n",
        "    train_acc_hist.append(train_acc)\n",
        "    test_acc_hist.append(test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BF161J_nAFsN",
        "outputId": "d9aec3a9-cc91-411a-f59f-d5cf1c40f373"
      },
      "id": "BF161J_nAFsN",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch  1 | loss 0.8285 | train acc 0.7183 | test acc 0.9132\n",
            "epoch  2 | loss 0.2501 | train acc 0.9274 | test acc 0.9506\n",
            "epoch  3 | loss 0.1861 | train acc 0.9473 | test acc 0.9660\n",
            "epoch  4 | loss 0.1708 | train acc 0.9516 | test acc 0.9616\n",
            "epoch  5 | loss 0.1559 | train acc 0.9552 | test acc 0.9712\n",
            "epoch  6 | loss 0.1387 | train acc 0.9606 | test acc 0.9730\n",
            "epoch  7 | loss 0.1331 | train acc 0.9621 | test acc 0.9743\n",
            "epoch  8 | loss 0.1331 | train acc 0.9628 | test acc 0.9692\n",
            "epoch  9 | loss 0.1250 | train acc 0.9649 | test acc 0.9830\n",
            "epoch 10 | loss 0.1219 | train acc 0.9664 | test acc 0.9766\n",
            "epoch 11 | loss 0.1188 | train acc 0.9673 | test acc 0.9656\n",
            "epoch 12 | loss 0.1202 | train acc 0.9666 | test acc 0.9789\n",
            "epoch 13 | loss 0.1149 | train acc 0.9685 | test acc 0.9725\n",
            "epoch 14 | loss 0.1118 | train acc 0.9689 | test acc 0.9726\n",
            "epoch 15 | loss 0.1128 | train acc 0.9687 | test acc 0.9779\n",
            "epoch 16 | loss 0.1075 | train acc 0.9707 | test acc 0.9780\n",
            "epoch 17 | loss 0.1045 | train acc 0.9716 | test acc 0.9725\n",
            "epoch 18 | loss 0.1025 | train acc 0.9714 | test acc 0.9772\n",
            "epoch 19 | loss 0.1040 | train acc 0.9711 | test acc 0.9823\n",
            "epoch 20 | loss 0.1005 | train acc 0.9728 | test acc 0.9810\n",
            "epoch 21 | loss 0.1002 | train acc 0.9735 | test acc 0.9807\n",
            "epoch 22 | loss 0.1020 | train acc 0.9726 | test acc 0.9795\n",
            "epoch 23 | loss 0.0979 | train acc 0.9729 | test acc 0.9812\n",
            "epoch 24 | loss 0.0950 | train acc 0.9735 | test acc 0.9737\n",
            "epoch 25 | loss 0.0939 | train acc 0.9740 | test acc 0.9823\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BNLeNet on CIFAR"
      ],
      "metadata": {
        "id": "dLV_L4WeKdyk"
      },
      "id": "dLV_L4WeKdyk"
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = d2l.Trainer(max_epochs=30, num_gpus=1)\n",
        "trainer.show_progress = True\n",
        "\n",
        "\n",
        "\n",
        "device = d2l.try_gpu()\n",
        "model = model.to(device)\n",
        "\n",
        "\n",
        "lr = getattr(model, \"lr\", 0.1)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "num_epochs = 25\n",
        "\n",
        "# --- buffers (no board yet) ---\n",
        "epochs_hist, train_loss_hist, train_acc_hist, test_acc_hist = [], [], [], []\n",
        "\n",
        "def evaluate_accuracy(model, loader, device):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in loader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X).argmax(1)\n",
        "            correct += (pred == y).sum().item()\n",
        "            total   += y.numel()\n",
        "    return correct / max(total, 1)\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    model.train()\n",
        "    sum_loss = 0.0\n",
        "    correct, total = 0, 0\n",
        "\n",
        "    for X, y in train_iter:\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        logits = model(X)\n",
        "        loss = loss_fn(logits, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        bs = y.numel()\n",
        "        sum_loss += loss.item() * bs\n",
        "        correct  += (logits.argmax(1) == y).sum().item()\n",
        "        total    += bs\n",
        "\n",
        "    train_loss = sum_loss / total\n",
        "    train_acc  = correct / total\n",
        "    test_acc   = evaluate_accuracy(model, test_iter, device)\n",
        "\n",
        "    # print now\n",
        "    print(f\"epoch {epoch:2d} | loss {train_loss:.4f} | \"\n",
        "          f\"train acc {train_acc:.4f} | test acc {test_acc:.4f}\", flush=True)\n",
        "\n",
        "    # buffer for plotting later\n",
        "    epochs_hist.append(epoch)\n",
        "    train_loss_hist.append(train_loss)\n",
        "    train_acc_hist.append(train_acc)\n",
        "    test_acc_hist.append(test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ch7mORfXKg1a",
        "outputId": "fb768264-73af-4b30-881d-b0fb50cd6551"
      },
      "id": "ch7mORfXKg1a",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch  1 | loss 1.5113 | train acc 0.4466 | test acc 0.4994\n",
            "epoch  2 | loss 1.1417 | train acc 0.5898 | test acc 0.5763\n",
            "epoch  3 | loss 1.0000 | train acc 0.6439 | test acc 0.6426\n",
            "epoch  4 | loss 0.9111 | train acc 0.6762 | test acc 0.6870\n",
            "epoch  5 | loss 0.8516 | train acc 0.6954 | test acc 0.7253\n",
            "epoch  6 | loss 0.8039 | train acc 0.7153 | test acc 0.6931\n",
            "epoch  7 | loss 0.7757 | train acc 0.7258 | test acc 0.6942\n",
            "epoch  8 | loss 0.7390 | train acc 0.7378 | test acc 0.7289\n",
            "epoch  9 | loss 0.7100 | train acc 0.7480 | test acc 0.7395\n",
            "epoch 10 | loss 0.6884 | train acc 0.7584 | test acc 0.7362\n",
            "epoch 11 | loss 0.6728 | train acc 0.7645 | test acc 0.7553\n",
            "epoch 12 | loss 0.6539 | train acc 0.7708 | test acc 0.7501\n",
            "epoch 13 | loss 0.6454 | train acc 0.7744 | test acc 0.7659\n",
            "epoch 14 | loss 0.6256 | train acc 0.7796 | test acc 0.7530\n",
            "epoch 15 | loss 0.6129 | train acc 0.7841 | test acc 0.7574\n",
            "epoch 16 | loss 0.6060 | train acc 0.7875 | test acc 0.7765\n",
            "epoch 17 | loss 0.5917 | train acc 0.7916 | test acc 0.7855\n",
            "epoch 18 | loss 0.5849 | train acc 0.7968 | test acc 0.7856\n",
            "epoch 19 | loss 0.5699 | train acc 0.7985 | test acc 0.7890\n",
            "epoch 20 | loss 0.5679 | train acc 0.7995 | test acc 0.7908\n",
            "epoch 21 | loss 0.5569 | train acc 0.8034 | test acc 0.7805\n",
            "epoch 22 | loss 0.5450 | train acc 0.8076 | test acc 0.7912\n",
            "epoch 23 | loss 0.5435 | train acc 0.8091 | test acc 0.7884\n",
            "epoch 24 | loss 0.5278 | train acc 0.8152 | test acc 0.7919\n",
            "epoch 25 | loss 0.5243 | train acc 0.8138 | test acc 0.7874\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BNLeNet on MNIST"
      ],
      "metadata": {
        "id": "Zw9WvKWcS1_O"
      },
      "id": "Zw9WvKWcS1_O"
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = d2l.Trainer(max_epochs=30, num_gpus=1)\n",
        "trainer.show_progress = True\n",
        "\n",
        "\n",
        "\n",
        "device = d2l.try_gpu()\n",
        "model = model.to(device)\n",
        "\n",
        "\n",
        "lr = getattr(model, \"lr\", 0.1)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "num_epochs = 25\n",
        "\n",
        "# --- buffers (no board yet) ---\n",
        "epochs_hist, train_loss_hist, train_acc_hist, test_acc_hist = [], [], [], []\n",
        "\n",
        "def evaluate_accuracy(model, loader, device):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in loader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X).argmax(1)\n",
        "            correct += (pred == y).sum().item()\n",
        "            total   += y.numel()\n",
        "    return correct / max(total, 1)\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    model.train()\n",
        "    sum_loss = 0.0\n",
        "    correct, total = 0, 0\n",
        "\n",
        "    for X, y in train_iter:\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        logits = model(X)\n",
        "        loss = loss_fn(logits, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        bs = y.numel()\n",
        "        sum_loss += loss.item() * bs\n",
        "        correct  += (logits.argmax(1) == y).sum().item()\n",
        "        total    += bs\n",
        "\n",
        "    train_loss = sum_loss / total\n",
        "    train_acc  = correct / total\n",
        "    test_acc   = evaluate_accuracy(model, test_iter, device)\n",
        "\n",
        "    # print now\n",
        "    print(f\"epoch {epoch:2d} | loss {train_loss:.4f} | \"\n",
        "          f\"train acc {train_acc:.4f} | test acc {test_acc:.4f}\", flush=True)\n",
        "\n",
        "    # buffer for plotting later\n",
        "    epochs_hist.append(epoch)\n",
        "    train_loss_hist.append(train_loss)\n",
        "    train_acc_hist.append(train_acc)\n",
        "    test_acc_hist.append(test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sL3nlJOSTCcg",
        "outputId": "a22959d9-2145-4253-d5e7-a2df1b37d337"
      },
      "id": "sL3nlJOSTCcg",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch  1 | loss 0.2689 | train acc 0.9126 | test acc 0.9656\n",
            "epoch  2 | loss 0.1043 | train acc 0.9673 | test acc 0.9721\n",
            "epoch  3 | loss 0.0859 | train acc 0.9725 | test acc 0.9737\n",
            "epoch  4 | loss 0.0746 | train acc 0.9764 | test acc 0.9813\n",
            "epoch  5 | loss 0.0679 | train acc 0.9780 | test acc 0.9809\n",
            "epoch  6 | loss 0.0616 | train acc 0.9805 | test acc 0.9834\n",
            "epoch  7 | loss 0.0586 | train acc 0.9808 | test acc 0.9836\n",
            "epoch  8 | loss 0.0551 | train acc 0.9825 | test acc 0.9860\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch  9 | loss 0.0512 | train acc 0.9831 | test acc 0.9835\n",
            "epoch 10 | loss 0.0503 | train acc 0.9838 | test acc 0.9878\n",
            "epoch 11 | loss 0.0487 | train acc 0.9844 | test acc 0.9857\n",
            "epoch 12 | loss 0.0476 | train acc 0.9846 | test acc 0.9858\n",
            "epoch 13 | loss 0.0429 | train acc 0.9861 | test acc 0.9841\n",
            "epoch 14 | loss 0.0447 | train acc 0.9858 | test acc 0.9884\n",
            "epoch 15 | loss 0.0417 | train acc 0.9867 | test acc 0.9889\n",
            "epoch 16 | loss 0.0430 | train acc 0.9862 | test acc 0.9888\n",
            "epoch 17 | loss 0.0412 | train acc 0.9863 | test acc 0.9892\n",
            "epoch 18 | loss 0.0397 | train acc 0.9871 | test acc 0.9868\n",
            "epoch 19 | loss 0.0373 | train acc 0.9879 | test acc 0.9885\n",
            "epoch 20 | loss 0.0381 | train acc 0.9879 | test acc 0.9881\n",
            "epoch 21 | loss 0.0362 | train acc 0.9881 | test acc 0.9903\n",
            "epoch 22 | loss 0.0368 | train acc 0.9885 | test acc 0.9861\n",
            "epoch 23 | loss 0.0369 | train acc 0.9877 | test acc 0.9901\n",
            "epoch 24 | loss 0.0354 | train acc 0.9885 | test acc 0.9909\n",
            "epoch 25 | loss 0.0343 | train acc 0.9891 | test acc 0.9881\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BNLeNet ResBlock on CIFAR"
      ],
      "metadata": {
        "id": "HFsrFGjmnBE9"
      },
      "id": "HFsrFGjmnBE9"
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = d2l.Trainer(max_epochs=30, num_gpus=1)\n",
        "trainer.show_progress = True\n",
        "\n",
        "\n",
        "\n",
        "device = d2l.try_gpu()\n",
        "model = model.to(device)\n",
        "\n",
        "\n",
        "lr = getattr(model, \"lr\", 0.1)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "num_epochs = 25\n",
        "\n",
        "# --- buffers (no board yet) ---\n",
        "epochs_hist, train_loss_hist, train_acc_hist, test_acc_hist = [], [], [], []\n",
        "\n",
        "def evaluate_accuracy(model, loader, device):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in loader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X).argmax(1)\n",
        "            correct += (pred == y).sum().item()\n",
        "            total   += y.numel()\n",
        "    return correct / max(total, 1)\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    model.train()\n",
        "    sum_loss = 0.0\n",
        "    correct, total = 0, 0\n",
        "\n",
        "    for X, y in train_iter:\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        logits = model(X)\n",
        "        loss = loss_fn(logits, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        bs = y.numel()\n",
        "        sum_loss += loss.item() * bs\n",
        "        correct  += (logits.argmax(1) == y).sum().item()\n",
        "        total    += bs\n",
        "\n",
        "    train_loss = sum_loss / total\n",
        "    train_acc  = correct / total\n",
        "    test_acc   = evaluate_accuracy(model, test_iter, device)\n",
        "\n",
        "    # print now\n",
        "    print(f\"epoch {epoch:2d} | loss {train_loss:.4f} | \"\n",
        "          f\"train acc {train_acc:.4f} | test acc {test_acc:.4f}\", flush=True)\n",
        "\n",
        "    # buffer for plotting later\n",
        "    epochs_hist.append(epoch)\n",
        "    train_loss_hist.append(train_loss)\n",
        "    train_acc_hist.append(train_acc)\n",
        "    test_acc_hist.append(test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6qiGvA9m_aH",
        "outputId": "8e8deac0-52eb-480e-d754-dc9efc64e1cf"
      },
      "id": "O6qiGvA9m_aH",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch  1 | loss 1.5770 | train acc 0.4205 | test acc 0.5221\n",
            "epoch  2 | loss 1.1697 | train acc 0.5755 | test acc 0.6045\n",
            "epoch  3 | loss 0.9480 | train acc 0.6608 | test acc 0.6731\n",
            "epoch  4 | loss 0.8189 | train acc 0.7106 | test acc 0.7247\n",
            "epoch  5 | loss 0.7350 | train acc 0.7424 | test acc 0.7090\n",
            "epoch  6 | loss 0.6861 | train acc 0.7595 | test acc 0.7412\n",
            "epoch  7 | loss 0.6319 | train acc 0.7796 | test acc 0.7582\n",
            "epoch  8 | loss 0.5956 | train acc 0.7923 | test acc 0.7850\n",
            "epoch  9 | loss 0.5633 | train acc 0.8019 | test acc 0.7814\n",
            "epoch 10 | loss 0.5421 | train acc 0.8108 | test acc 0.7819\n",
            "epoch 11 | loss 0.5223 | train acc 0.8178 | test acc 0.7973\n",
            "epoch 12 | loss 0.4949 | train acc 0.8282 | test acc 0.8131\n",
            "epoch 13 | loss 0.4747 | train acc 0.8356 | test acc 0.8166\n",
            "epoch 14 | loss 0.4649 | train acc 0.8352 | test acc 0.8028\n",
            "epoch 15 | loss 0.4493 | train acc 0.8437 | test acc 0.8212\n",
            "epoch 16 | loss 0.4313 | train acc 0.8491 | test acc 0.8076\n",
            "epoch 17 | loss 0.4240 | train acc 0.8522 | test acc 0.8337\n",
            "epoch 18 | loss 0.4153 | train acc 0.8539 | test acc 0.8203\n",
            "epoch 19 | loss 0.4027 | train acc 0.8592 | test acc 0.8247\n",
            "epoch 20 | loss 0.3957 | train acc 0.8611 | test acc 0.8369\n",
            "epoch 21 | loss 0.3900 | train acc 0.8629 | test acc 0.8333\n",
            "epoch 22 | loss 0.3768 | train acc 0.8681 | test acc 0.8375\n",
            "epoch 23 | loss 0.3632 | train acc 0.8726 | test acc 0.8320\n",
            "epoch 24 | loss 0.3543 | train acc 0.8766 | test acc 0.8293\n",
            "epoch 25 | loss 0.3531 | train acc 0.8747 | test acc 0.8345\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BNLeNet ResBlock on MNIST"
      ],
      "metadata": {
        "id": "Bf1uc8dqpTX2"
      },
      "id": "Bf1uc8dqpTX2"
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = d2l.Trainer(max_epochs=30, num_gpus=1)\n",
        "trainer.show_progress = True\n",
        "\n",
        "\n",
        "\n",
        "device = d2l.try_gpu()\n",
        "model = model.to(device)\n",
        "\n",
        "\n",
        "lr = getattr(model, \"lr\", 0.1)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "num_epochs = 25\n",
        "\n",
        "# --- buffers (no board yet) ---\n",
        "epochs_hist, train_loss_hist, train_acc_hist, test_acc_hist = [], [], [], []\n",
        "\n",
        "def evaluate_accuracy(model, loader, device):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in loader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X).argmax(1)\n",
        "            correct += (pred == y).sum().item()\n",
        "            total   += y.numel()\n",
        "    return correct / max(total, 1)\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    model.train()\n",
        "    sum_loss = 0.0\n",
        "    correct, total = 0, 0\n",
        "\n",
        "    for X, y in train_iter:\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        logits = model(X)\n",
        "        loss = loss_fn(logits, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        bs = y.numel()\n",
        "        sum_loss += loss.item() * bs\n",
        "        correct  += (logits.argmax(1) == y).sum().item()\n",
        "        total    += bs\n",
        "\n",
        "    train_loss = sum_loss / total\n",
        "    train_acc  = correct / total\n",
        "    test_acc   = evaluate_accuracy(model, test_iter, device)\n",
        "\n",
        "    # print now\n",
        "    print(f\"epoch {epoch:2d} | loss {train_loss:.4f} | \"\n",
        "          f\"train acc {train_acc:.4f} | test acc {test_acc:.4f}\", flush=True)\n",
        "\n",
        "    # buffer for plotting later\n",
        "    epochs_hist.append(epoch)\n",
        "    train_loss_hist.append(train_loss)\n",
        "    train_acc_hist.append(train_acc)\n",
        "    test_acc_hist.append(test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85i0OBw0pSO5",
        "outputId": "50f17e13-9fb2-42eb-a98f-a9d92443d561"
      },
      "id": "85i0OBw0pSO5",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch  1 | loss 0.2780 | train acc 0.9086 | test acc 0.9638\n",
            "epoch  2 | loss 0.0941 | train acc 0.9708 | test acc 0.9847\n",
            "epoch  3 | loss 0.0715 | train acc 0.9778 | test acc 0.9810\n",
            "epoch  4 | loss 0.0644 | train acc 0.9795 | test acc 0.9727\n",
            "epoch  5 | loss 0.0577 | train acc 0.9823 | test acc 0.9885\n",
            "epoch  6 | loss 0.0514 | train acc 0.9837 | test acc 0.9861\n",
            "epoch  7 | loss 0.0491 | train acc 0.9848 | test acc 0.9869\n",
            "epoch  8 | loss 0.0464 | train acc 0.9853 | test acc 0.9899\n",
            "epoch  9 | loss 0.0423 | train acc 0.9866 | test acc 0.9877\n",
            "epoch 10 | loss 0.0401 | train acc 0.9869 | test acc 0.9892\n",
            "epoch 11 | loss 0.0413 | train acc 0.9869 | test acc 0.9887\n",
            "epoch 12 | loss 0.0374 | train acc 0.9881 | test acc 0.9881\n",
            "epoch 13 | loss 0.0382 | train acc 0.9877 | test acc 0.9881\n",
            "epoch 14 | loss 0.0347 | train acc 0.9890 | test acc 0.9914\n",
            "epoch 15 | loss 0.0325 | train acc 0.9896 | test acc 0.9865\n",
            "epoch 16 | loss 0.0335 | train acc 0.9894 | test acc 0.9890\n",
            "epoch 17 | loss 0.0303 | train acc 0.9904 | test acc 0.9921\n",
            "epoch 18 | loss 0.0318 | train acc 0.9898 | test acc 0.9910\n",
            "epoch 19 | loss 0.0299 | train acc 0.9900 | test acc 0.9901\n",
            "epoch 20 | loss 0.0282 | train acc 0.9911 | test acc 0.9868\n",
            "epoch 21 | loss 0.0290 | train acc 0.9906 | test acc 0.9886\n",
            "epoch 22 | loss 0.0284 | train acc 0.9905 | test acc 0.9899\n",
            "epoch 23 | loss 0.0271 | train acc 0.9910 | test acc 0.9905\n",
            "epoch 24 | loss 0.0264 | train acc 0.9909 | test acc 0.9931\n",
            "epoch 25 | loss 0.0256 | train acc 0.9915 | test acc 0.9909\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b87fd00",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 638
        },
        "id": "2b87fd00",
        "outputId": "945f9351-c337-4e3f-c0c2-fa3a1691b1ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# --- draw once at the very end ---\n",
        "\n",
        "print(\"\\n\\n\")\n",
        "board = d2l.ProgressBoard()\n",
        "board.x = 'epoch'\n",
        "board.xlabel = 'epoch'\n",
        "for e, v in zip(epochs_hist, train_loss_hist):\n",
        "    board.draw(e, v, 'train loss')\n",
        "for e, v in zip(epochs_hist, train_acc_hist):\n",
        "    board.draw(e, v, 'train acc')\n",
        "for e, v in zip(epochs_hist, test_acc_hist):\n",
        "    board.draw(e, v, 'test acc')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f396baf",
      "metadata": {
        "origin_pos": 20,
        "id": "4f396baf"
      },
      "source": [
        "## Summary\n",
        "\n",
        "We have made significant progress in this chapter. We moved from the MLPs of the 1980s to the CNNs of the 1990s and early 2000s. The architectures proposed, e.g., in the form of LeNet-5 remain meaningful, even to this day. It is worth comparing the error rates on Fashion-MNIST achievable with LeNet-5 both to the very best possible with MLPs (:numref:`sec_mlp-implementation`) and those with significantly more advanced architectures such as ResNet (:numref:`sec_resnet`). LeNet is much more similar to the latter than to the former. One of the primary differences, as we shall see, is that greater amounts of computation enabled significantly more complex architectures.\n",
        "\n",
        "A second difference is the relative ease with which we were able to implement LeNet. What used to be an engineering challenge worth months of C++ and assembly code, engineering to improve SN, an early Lisp-based deep learning tool :cite:`Bottou.Le-Cun.1988`, and finally experimentation with models can now be accomplished in minutes. It is this incredible productivity boost that has democratized deep learning model development tremendously. In the next chapter we will journey down this rabbit to hole to see where it takes us.\n",
        "\n",
        "## Exercises\n",
        "\n",
        "1. Let's modernize LeNet. Implement and test the following changes:\n",
        "    1. Replace average pooling with max-pooling.\n",
        "    1. Replace the softmax layer with ReLU.\n",
        "1. Try to change the size of the LeNet style network to improve its accuracy in addition to max-pooling and ReLU.\n",
        "    1. Adjust the convolution window size.\n",
        "    1. Adjust the number of output channels.\n",
        "    1. Adjust the number of convolution layers.\n",
        "    1. Adjust the number of fully connected layers.\n",
        "    1. Adjust the learning rates and other training details (e.g., initialization and number of epochs).\n",
        "1. Try out the improved network on the original MNIST dataset.\n",
        "1. Display the activations of the first and second layer of LeNet for different inputs (e.g., sweaters and coats).\n",
        "1. What happens to the activations when you feed significantly different images into the network (e.g., cats, cars, or even random noise)?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44e195cb",
      "metadata": {
        "origin_pos": 22,
        "tab": [
          "pytorch"
        ],
        "id": "44e195cb"
      },
      "source": [
        "[Discussions](https://discuss.d2l.ai/t/74)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05e7bbe7-9354-450c-b560-a81b91c777d9",
      "metadata": {
        "id": "05e7bbe7-9354-450c-b560-a81b91c777d9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f37bbaf-b482-469f-b016-36e12d00300d",
      "metadata": {
        "id": "9f37bbaf-b482-469f-b016-36e12d00300d"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "celltoolbar": "Tags",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    },
    "required_libs": [],
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}